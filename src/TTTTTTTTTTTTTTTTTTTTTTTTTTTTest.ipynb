{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import os, sys, shutil\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from time import sleep\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "import torch.distributions as D\n",
    "\n",
    "import data as data_\n",
    "import nn as nn_\n",
    "import utils\n",
    "from mfp_utils import *\n",
    "\n",
    "from experiments import cutils\n",
    "from nde import distributions, flows, transforms\n",
    "\n",
    "import klampt\n",
    "from klampt.plan import cspace, robotplanning\n",
    "from klampt.plan.robotcspace import RobotCSpace\n",
    "from klampt.model import collide\n",
    "from klampt.model.trajectory import RobotTrajectory\n",
    "from klampt.io import resource\n",
    "\n",
    "from mfg_models import construct_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "parser = argparse.ArgumentParser()\n",
    "# 模拟命令行输入\n",
    "sys.argv = ['--exp_name 2D --linear_transform_type lu_no_perm --reg_OT_dir gen --num_training_steps 1000 --base_transform_type rq-coupling --tail_bound 5 --OT_part block_CL_no_perm --lbd_OT 2e-1 --gaussian_multi_dim 2 --num_train_data 2000 --LU_last false --NF_loss jeffery --lr_schedule adaptive --train_batch_size 2048 --learning_rate 1e-3']\n",
    "# data\n",
    "parser.add_argument('--exp_name', type=str, default='2D')\n",
    "parser.add_argument('--dataset_name', type=str, default='gaussian_mixture',\n",
    "                    choices=['gaussian_mixture', 'crowd_motion_gaussian', 'crowd_motion_gaussian_close'\n",
    "                            'crowd_motion_gaussian_nonsmooth_obs', 'moons', \n",
    "                            'gaussian', '2spirals', 'checkerboard',\n",
    "                            'power', 'gas', 'hepmass', 'miniboone', 'bsds300',\n",
    "                            'robot_1'],\n",
    "                    help='Name of dataset to use.')\n",
    "parser.add_argument('--val_frac', type=float, default=1.,\n",
    "                    help='Fraction of validation set to use.')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-3,\n",
    "                    help='Learning rate for optimizer.')\n",
    "parser.add_argument('--num_training_steps', type=int, default=1000,\n",
    "                    help='Number of total training steps.')\n",
    "# parser.add_argument('--anneal_learning_rate', type=int, default=1,\n",
    "#                     choices=[0, 1],\n",
    "#                     help='Whether to anneal the learning rate.')\n",
    "parser.add_argument('--lr_schedule', type=str, default='adaptive',\n",
    "                    choices=['none', 'cyclic', 'adaptive'])\n",
    "parser.add_argument('--grad_norm_clip_value', type=float, default=5.,\n",
    "                    help='Value by which to clip norm of gradients.')\n",
    "parser.add_argument('--lbd_reg', type=float, default=0)\n",
    "\n",
    "\n",
    "# flow details\n",
    "parser.add_argument('--base_transform_type', type=str, default='rq-coupling',\n",
    "                    choices=['affine-coupling', 'quadratic-coupling', 'rq-coupling',\n",
    "                             'affine-autoregressive', 'quadratic-autoregressive',\n",
    "                             'rq-autoregressive'],\n",
    "                    help='Type of transform to use between linear layers.')\n",
    "parser.add_argument('--linear_transform_type', type=str, default='lu_no_perm',\n",
    "                    choices=['permutation', 'lu', 'svd', 'lu_no_perm'],\n",
    "                    help='Type of linear transform to use.')\n",
    "parser.add_argument('--num_flow_steps', type=int, default=7,\n",
    "                    help='Number of blocks to use in flow.')\n",
    "parser.add_argument('--hidden_features', type=int, default=256,\n",
    "                    help='Number of hidden features to use in coupling/autoregressive nets.')\n",
    "parser.add_argument('--tail_bound', type=float, default=5,\n",
    "                    help='Box is on [-bound, bound]^2')\n",
    "parser.add_argument('--num_bins', type=int, default=8,\n",
    "                    help='Number of bins to use for piecewise transforms.')\n",
    "parser.add_argument('--num_transform_blocks', type=int, default=2,\n",
    "                    help='Number of blocks to use in coupling/autoregressive nets.')\n",
    "parser.add_argument('--use_batch_norm', type=int, default=0,\n",
    "                    choices=[0, 1],\n",
    "                    help='Whether to use batch norm in coupling/autoregressive nets.')\n",
    "parser.add_argument('--dropout_probability', type=float, default=0.25,\n",
    "                    help='Dropout probability for coupling/autoregressive nets.')\n",
    "parser.add_argument('--apply_unconditional_transform', type=int, default=1,\n",
    "                    choices=[0, 1],\n",
    "                    help='Whether to unconditionally transform \\'identity\\' '\n",
    "                         'features in coupling layer.')\n",
    "parser.add_argument('--base_net_act', type=str, default='relu',\n",
    "                    choices=['relu', 'tanh'])\n",
    "\n",
    "# logging and checkpoints\n",
    "parser.add_argument('--monitor_interval', type=int, default=250,\n",
    "                    help='Interval in steps at which to report training stats.')\n",
    "\n",
    "# reproducibility\n",
    "parser.add_argument('--seed', type=int, default=1638128,\n",
    "                    help='Random seed for PyTorch and NumPy.')\n",
    "\n",
    "# MFG\n",
    "parser.add_argument('--gaussian_multi_dim', type=int, default=2)\n",
    "parser.add_argument('--gaussian_multi_a',   type=float, default=10.)\n",
    "parser.add_argument('--num_train_data',     type=int, default=2000)\n",
    "parser.add_argument('--num_val_data',       type=int, default=10000)\n",
    "parser.add_argument('--num_test_data',      type=int, default=10000)\n",
    "parser.add_argument('--train_batch_size',   type=int, default=2048)\n",
    "parser.add_argument('--val_batch_size',     type=int, default=512)\n",
    "parser.add_argument('--test_batch_size',    type=int, default=512)\n",
    "parser.add_argument('--lbd_OT',             type=float, default=2e-1)\n",
    "parser.add_argument('--lbd_F',              type=float, default=0)\n",
    "parser.add_argument('--lbd_F_E',            type=float, default=0.01)\n",
    "parser.add_argument('--lbd_F_P',            type=float, default=1)\n",
    "parser.add_argument('--reg_OT_dir',         type=str, default='gen', choices=['gen', 'norm'])\n",
    "parser.add_argument('--OT_comp',            type=str, default='trajectory', choices=['trajectory', 'monge'])\n",
    "parser.add_argument('--OT_part',            type=str, default='block_CL_no_perm', choices=['block', 'block_CL_no_perm', 'module'])\n",
    "parser.add_argument('--interaction',        type=lambda x: (str(x).lower() == 'true'), default=False)\n",
    "parser.add_argument('--LU_last',            type=lambda x: (str(x).lower() == 'true'), default=False)\n",
    "parser.add_argument('--NF_loss',            type=str, default='jeffery', choices=[\n",
    "                                            'KL_sampling', 'KL_density', 'jeffery'])\n",
    "parser.add_argument('--val_score',          type=str, default='loss', choices=[\n",
    "                                            'loss', 'L', 'G', 'F'])\n",
    "parser.add_argument('--mixture_base',       type=str, default='gaussian', choices=[\n",
    "                                            'gaussian', 'gaussian_mixture'])\n",
    "parser.add_argument('--mixture_weight',     type=str, default='identical', choices=[\n",
    "                                            'identical', 'undersample_one'])  \n",
    "parser.add_argument('--F_ld_weight',        type=str, default='identical', choices=['identical'])                                                                          \n",
    "parser.add_argument('--disc_scheme',        type=str, default='forward', choices=[\n",
    "                                            'forward', 'centered', 'forward_2nd',\n",
    "                                            'FD4_simp', 'FD1_simp', 'FD4_simp_symmetric'])\n",
    "parser.add_argument('--NF_model',           type=str, default='default', choices=[\n",
    "                                            'default', 'single_flow'])                                     \n",
    "parser.add_argument('--obs_nonsmooth_val',  type=float, default=100.)\n",
    "parser.add_argument('--interp_hist',        type=lambda x: (str(x).lower() == 'true'), default=False)\n",
    "parser.add_argument('--n_interp',           type=int, default=5, \n",
    "                    help='Number of interpolated points inserted between flow points to better approximate MFG costs')\n",
    "## robotics\n",
    "parser.add_argument('--robot_init_pos',     type=str, default='default', choices=['default', \n",
    "                                            'under_table', 'under_table_2', 'under_table_3', 'under_table_4',\n",
    "                                            'under_table_hard']) \n",
    "parser.add_argument('--robot_term_pos',     type=str, default='cup', choices=['cup']) \n",
    "parser.add_argument('--robot_var',          type=float, default=1e-5)\n",
    "parser.add_argument('--robot_obs_val',      type=float, default=1e2)\n",
    "parser.add_argument('--robot_1_obs',        type=str, default='thick_sigmoid_B=256')\n",
    "parser.add_argument('--robot_1_obs_l',      type=int, default=3)\n",
    "parser.add_argument('--robot_1_obs_act',    type=str, default='relu', choices=['relu', 'tanh'])\n",
    "parser.add_argument('--robot_1_base_dist',  type=str, default='default', choices=['default', 'two_init'])\n",
    "parser.add_argument('--obs_robot_sig',      type=lambda x: (str(x).lower() == 'true'), default=True) \n",
    "\n",
    "# misc.\n",
    "parser.add_argument('--plotting_subset',    type=int, default=10000)\n",
    "parser.add_argument('--load_best_val',      type=lambda x: (str(x).lower() == 'true'), default=False)\n",
    "parser.add_argument('--compute_lip_bound',  type=lambda x: (str(x).lower() == 'true'), default=False)\n",
    "parser.add_argument('--save_train_traj',    type=lambda x: (str(x).lower() == 'true'), default=False)\n",
    "parser.add_argument('--syn_noise',          type=float, default=0.1)\n",
    "parser.add_argument('--marker_size',        type=float, default=5)\n",
    "parser.add_argument('--color',              type=str, default='order', choices=[\n",
    "                                            'order', 'radius'])\n",
    "parser.add_argument('--tabular_subset',     type=lambda x: (str(x).lower() == 'true'), default=False)\n",
    "parser.add_argument('--tensor_type',        type=str, default='float', choices=['float', 'double']) \n",
    "\n",
    "args = parser.parse_args()\n",
    "args = sanitize_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/am2l/anaconda3/envs/env1/lib/python3.11/site-packages/torch/__init__.py:614: UserWarning: torch.set_default_tensor_type() is deprecated as of PyTorch 2.1, please use torch.set_default_dtype() and torch.set_default_device() as alternatives. (Triggered internally at ../torch/csrc/tensor/python_tensor.cpp:451.)\n",
      "  _C._set_default_tensor_type(t)\n"
     ]
    }
   ],
   "source": [
    "# =================================================================================== #\n",
    "#                                       Meta                                          #\n",
    "# =================================================================================== #\n",
    "\n",
    "os.environ['DATAROOT'] = 'experiments/dataset/data/'\n",
    "os.environ['SLURM_JOB_ID'] = '1'\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "device = torch.device('cuda')\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 检查CUDA是否可用，然后选择设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Current device:\", device)\n",
    "\n",
    "import torch\n",
    "\n",
    "# 假设 'cuda' 可用\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    generator = torch.Generator(device=device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    generator = torch.Generator(device=device)\n",
    "\n",
    "# 使用这个生成器与 torch.randperm\n",
    "rand_indices = torch.randperm(1, generator=generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================== #\n",
    "#                                       Dataset                                       #\n",
    "# =================================================================================== #\n",
    "target_dist = None\n",
    "space = None\n",
    "num_mixtures = 8\n",
    "weight = D.Categorical(torch.ones(num_mixtures,).to(device))\n",
    "X_train, _, _, train_loader, val_loader, test_loader, target_dist = make_gaussian_mixture_data(args.mixture_base, args.gaussian_multi_dim, args.num_train_data, \\\n",
    "    args.num_val_data, args.num_test_data, args.train_batch_size, args.val_batch_size, args.test_batch_size, weight=weight)\n",
    "train_generator = data_.batch_generator(train_loader)\n",
    "train_loader\n",
    "test_batch      = next(iter(train_loader)).to(device)\n",
    "features        = args.gaussian_multi_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeTransform(\n",
      "  (_transforms): ModuleList(\n",
      "    (0): LULinear()\n",
      "    (1-2): 2 x PiecewiseRationalQuadraticCouplingTransform(\n",
      "      (transform_net): ResidualNet(\n",
      "        (initial_layer): Linear(in_features=1, out_features=256, bias=True)\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x ResidualBlock(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.25, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (final_layer): Linear(in_features=256, out_features=23, bias=True)\n",
      "      )\n",
      "      (unconditional_transform): PiecewiseRationalQuadraticCDF()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "There are 3774638 trainable parameters in this model.\n"
     ]
    }
   ],
   "source": [
    "# =================================================================================== #\n",
    "#                                       Model                                         #\n",
    "# =================================================================================== #\n",
    "\n",
    "\n",
    "# methods for chaining together the flow transformations\n",
    "def create_linear_transform():\n",
    "    return transforms.LULinear(features, identity_init=True)\n",
    "\n",
    "\n",
    "def create_base_transform(i):\n",
    "    act = F.relu\n",
    "\n",
    "\n",
    "    return transforms.PiecewiseRationalQuadraticCouplingTransform(\n",
    "        mask=utils.create_alternating_binary_mask(features, even=(i % 2 == 0)),\n",
    "        transform_net_create_fn=lambda in_features, out_features: nn_.ResidualNet(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "            hidden_features=args.hidden_features,\n",
    "            context_features=None,\n",
    "            num_blocks=args.num_transform_blocks,\n",
    "            activation=act,\n",
    "            dropout_probability=args.dropout_probability,\n",
    "            use_batch_norm=args.use_batch_norm\n",
    "        ),\n",
    "        num_bins=args.num_bins,\n",
    "        tails='linear',\n",
    "        tail_bound=args.tail_bound,\n",
    "        apply_unconditional_transform=args.apply_unconditional_transform\n",
    "    )\n",
    "\n",
    "def create_transform():\n",
    "    flows = [\n",
    "        transforms.CompositeTransform([\n",
    "            create_linear_transform(),\n",
    "            create_base_transform(i),\n",
    "            create_base_transform(i+1)\n",
    "        ]) for i in range(0, 2*args.num_flow_steps, 2)\n",
    "    ]\n",
    "\n",
    "    # flows = [\n",
    "    #     transforms.CompositeTransform([\n",
    "    #         create_linear_transform(),\n",
    "    #         create_base_transform(1)\n",
    "    #     ])\n",
    "    # ]\n",
    "\n",
    "    print((flows[0]))\n",
    "    K = args.num_flow_steps\n",
    "    transform = transforms.CompositeTransform(flows)\n",
    "\n",
    "    return transform, K\n",
    "\n",
    "# base dist\n",
    "\n",
    "cov          = 0.0625 * torch.eye(features).to(device)\n",
    "mean         = torch.zeros(features).to(device)\n",
    "distribution = distributions.MultivarNormal((features,), mean=mean, cov=cov)\n",
    "\n",
    "# create flows\n",
    "transform, K = create_transform()\n",
    "flow = flows.Flow(transform, distribution).to(device)\n",
    "n_params = utils.get_num_parameters(flow)\n",
    "print('There are {} trainable parameters in this model.'.format(n_params))\n",
    "\n",
    "# create optimizer\n",
    "optimizer = optim.Adam(flow.parameters(), lr=args.learning_rate, weight_decay=args.lbd_reg)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CompositeTransform(\n",
      "  (_transforms): ModuleList(\n",
      "    (0): LULinear()\n",
      "    (1-2): 2 x PiecewiseRationalQuadraticCouplingTransform(\n",
      "      (transform_net): ResidualNet(\n",
      "        (initial_layer): Linear(in_features=1, out_features=256, bias=True)\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x ResidualBlock(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.25, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (final_layer): Linear(in_features=256, out_features=23, bias=True)\n",
      "      )\n",
      "      (unconditional_transform): PiecewiseRationalQuadraticCDF()\n",
      "    )\n",
      "  )\n",
      "), CompositeTransform(\n",
      "  (_transforms): ModuleList(\n",
      "    (0): LULinear()\n",
      "    (1-2): 2 x PiecewiseRationalQuadraticCouplingTransform(\n",
      "      (transform_net): ResidualNet(\n",
      "        (initial_layer): Linear(in_features=1, out_features=256, bias=True)\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x ResidualBlock(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.25, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (final_layer): Linear(in_features=256, out_features=23, bias=True)\n",
      "      )\n",
      "      (unconditional_transform): PiecewiseRationalQuadraticCDF()\n",
      "    )\n",
      "  )\n",
      "), CompositeTransform(\n",
      "  (_transforms): ModuleList(\n",
      "    (0): LULinear()\n",
      "    (1-2): 2 x PiecewiseRationalQuadraticCouplingTransform(\n",
      "      (transform_net): ResidualNet(\n",
      "        (initial_layer): Linear(in_features=1, out_features=256, bias=True)\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x ResidualBlock(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.25, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (final_layer): Linear(in_features=256, out_features=23, bias=True)\n",
      "      )\n",
      "      (unconditional_transform): PiecewiseRationalQuadraticCDF()\n",
      "    )\n",
      "  )\n",
      "), CompositeTransform(\n",
      "  (_transforms): ModuleList(\n",
      "    (0): LULinear()\n",
      "    (1-2): 2 x PiecewiseRationalQuadraticCouplingTransform(\n",
      "      (transform_net): ResidualNet(\n",
      "        (initial_layer): Linear(in_features=1, out_features=256, bias=True)\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x ResidualBlock(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.25, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (final_layer): Linear(in_features=256, out_features=23, bias=True)\n",
      "      )\n",
      "      (unconditional_transform): PiecewiseRationalQuadraticCDF()\n",
      "    )\n",
      "  )\n",
      "), CompositeTransform(\n",
      "  (_transforms): ModuleList(\n",
      "    (0): LULinear()\n",
      "    (1-2): 2 x PiecewiseRationalQuadraticCouplingTransform(\n",
      "      (transform_net): ResidualNet(\n",
      "        (initial_layer): Linear(in_features=1, out_features=256, bias=True)\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x ResidualBlock(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.25, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (final_layer): Linear(in_features=256, out_features=23, bias=True)\n",
      "      )\n",
      "      (unconditional_transform): PiecewiseRationalQuadraticCDF()\n",
      "    )\n",
      "  )\n",
      "), CompositeTransform(\n",
      "  (_transforms): ModuleList(\n",
      "    (0): LULinear()\n",
      "    (1-2): 2 x PiecewiseRationalQuadraticCouplingTransform(\n",
      "      (transform_net): ResidualNet(\n",
      "        (initial_layer): Linear(in_features=1, out_features=256, bias=True)\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x ResidualBlock(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.25, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (final_layer): Linear(in_features=256, out_features=23, bias=True)\n",
      "      )\n",
      "      (unconditional_transform): PiecewiseRationalQuadraticCDF()\n",
      "    )\n",
      "  )\n",
      "), CompositeTransform(\n",
      "  (_transforms): ModuleList(\n",
      "    (0): LULinear()\n",
      "    (1-2): 2 x PiecewiseRationalQuadraticCouplingTransform(\n",
      "      (transform_net): ResidualNet(\n",
      "        (initial_layer): Linear(in_features=1, out_features=256, bias=True)\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x ResidualBlock(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.25, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (final_layer): Linear(in_features=256, out_features=23, bias=True)\n",
      "      )\n",
      "      (unconditional_transform): PiecewiseRationalQuadraticCDF()\n",
      "    )\n",
      "  )\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "# KKKKKKKK = nn.ModuleList(transforms)\n",
    "len(list(transform._transforms)[::-1])\n",
    "print(list(transform._transforms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Flow(\n",
       "  (_transform): CompositeTransform(\n",
       "    (_transforms): ModuleList(\n",
       "      (0-6): 7 x CompositeTransform(\n",
       "        (_transforms): ModuleList(\n",
       "          (0): LULinear()\n",
       "          (1-2): 2 x PiecewiseRationalQuadraticCouplingTransform(\n",
       "            (transform_net): ResidualNet(\n",
       "              (initial_layer): Linear(in_features=1, out_features=256, bias=True)\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x ResidualBlock(\n",
       "                  (linear_layers): ModuleList(\n",
       "                    (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.25, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (final_layer): Linear(in_features=256, out_features=23, bias=True)\n",
       "            )\n",
       "            (unconditional_transform): PiecewiseRationalQuadraticCDF()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_distribution): MultivarNormal()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(K)\n",
    "flow\n",
    "# \"\"\"\n",
    "# Flow 类是一个包含变换和基础分布的模型。\n",
    "# 它通常用于定义一个可逆的变换链（CompositeTransform），\n",
    "# 以及一个简单的概率分布（_distribution），在这个例子中是多变量正态分布（MultivarNormal()）\n",
    "# \"\"\"\n",
    "\n",
    "# \"\"\"\n",
    "# CompositeTransform：这是一个复合变换，它将多个变换组合在一起。\n",
    "# 在此例子中，它包含了一个模块列表（ModuleList），其中每个模块也是一个 CompositeTransform。\n",
    "\n",
    "# ModuleList (0-9)：ModuleList 包含了10个 CompositeTransform 对象，这表明模型中有10个相似的流程步骤或层，每个都执行相似的变换。\n",
    "# \"\"\"\n",
    "\n",
    "# \"\"\" \n",
    "# 每个 CompositeTransform 包括：\n",
    "\n",
    "# LULinear()：这是一个线性变换，通常用于进行可逆的线性操作，如LU分解。\n",
    "\n",
    "# PiecewiseRationalQuadraticCouplingTransform (1-2)：包括两个此类型的变换，这类变换通常用于在保持一部分输入不变的同时，通过一种复杂的函数（在这里是由 ResidualNet 实现）修改输入的其余部分。\n",
    "\n",
    "#     transform_net：一个 ResidualNet 网络，用于根据输入特征生成变换的参数。这个网络包括：\n",
    "#         initial_layer：输入层，一个线性变换，将输入特征从1维扩展到256维。\n",
    "#         blocks：包含多个 ResidualBlock，每个块包括两个线性层和一个Dropout层，用于增加模型的非线性和防止过拟合。\n",
    "#         final_layer：输出层，另一个线性变换，将特征从256维压缩到23维。\n",
    "#     unconditional_transform：一个 PiecewiseRationalQuadraticCDF，用于在变换中实现无条件的、复杂的可逆函数。\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 设置图形\n",
    "def setup_figure(rows, cols, figsize=(18, 6)):\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=figsize)\n",
    "    return fig, axs\n",
    "\n",
    "# 绘制矢量场\n",
    "def plot_quiver(ax, data, last_data, title):\n",
    "    U = data[:, 0] - last_data[:, 0]\n",
    "    V = data[:, 1] - last_data[:, 1]\n",
    "    ax.quiver(last_data[:, 0], last_data[:, 1], U, V, scale=10, scale_units='width', color='b', width=0.005)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "\n",
    "# 绘制散点图\n",
    "def plot_scatter(ax, data, title):\n",
    "    ax.plot(data[:, 0], data[:, 1], '.')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "\n",
    "# 绘制密度图\n",
    "def plot_density(ax, data, title):\n",
    "    sns.kdeplot(x=data[:, 0], y=data[:, 1], fill=True, thresh=0, levels=100, cmap=\"viridis\", ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "\n",
    "# 主绘图函数\n",
    "def main_plot(part, z_0):\n",
    "    fig1, ax1 = setup_figure(2, 5)\n",
    "    fig2, ax2 = setup_figure(2, 5)\n",
    "    fig3, ax3 = setup_figure(2, 5)\n",
    "\n",
    "    last_x = z_0.data.cpu().numpy()\n",
    "    for i in range(8):\n",
    "        x_plot = part[:, i, :].data.cpu().numpy()\n",
    "        plot_quiver(ax1.flatten()[i], x_plot, last_x, f'Time step {i}')\n",
    "        plot_scatter(ax2.flatten()[i], x_plot, f'Time step {i}')\n",
    "        plot_density(ax3.flatten()[i], x_plot, f'Time step {i}')\n",
    "        last_x = x_plot\n",
    "\n",
    "    for fig in [fig1, fig2, fig3]:\n",
    "        fig.tight_layout()\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "# z_K, ld_gen, OT_cost_gen, hist_gen, hist_ld_gen, z_0 = flow.sample(args.train_batch_size)\n",
    "# hist = hist_gen\n",
    "# part = torch.cat(hist).reshape(len(hist), hist[0].shape[0], hist[0].shape[-1]).permute(1,0,2)\n",
    "# K = part.shape[1]\n",
    "# I = torch.arange(0, K, step=4)\n",
    "# part = part[:,I,:]\n",
    "\n",
    "# main_plot(part, z_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_K, ld_gen, OT_cost_gen, hist_gen, hist_ld_gen, z_0 = flow.sample(args.train_batch_size)\n",
    "# data = hist_gen[28]\n",
    "# data.requires_grad_(True)\n",
    "\n",
    "# for N in range(1,30):\n",
    "#     # 初始化结果为 logprob\n",
    "#     log_density = torch.zeros_like(hist_ld_gen[0])\n",
    "\n",
    "#     # 累加操作\n",
    "#     for idx in range(N):\n",
    "#         log_density += hist_ld_gen[idx] # 用 squeeze() 去除单一维度\n",
    "\n",
    "#     log_density = torch.sum(log_density)\n",
    "\n",
    "#     # log_density = torch.sum(torch.sum(torch.stack(hist_ld_norm[:N]), dim=0))\n",
    "#     # # return log_prob + logabsdet, log_prob, logabsdet, hist, hist_ld, OT_cost, noise\n",
    "#     grad = torch.autograd.grad(outputs=log_density, inputs=data, create_graph=True, grad_outputs=torch.ones_like(log_density), allow_unused=True)[0]\n",
    "#     if grad is not None:\n",
    "#         print('????????????', N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2048, 2])\n",
      "29 torch.Size([2048, 2]) 29 torch.Size([2048, 2])\n",
      "29 torch.Size([2048, 1]) 29 torch.Size([2048, 1])\n",
      "sample tensor(0., grad_fn=<SumBackward0>) tensor(0.)\n",
      "inverse tensor(0., grad_fn=<SumBackward0>) tensor(0., grad_fn=<SumBackward0>)\n",
      "log, i 0 tensor(0.) tensor(318.8028, grad_fn=<SumBackward0>)\n",
      "log, i 1 tensor(0.) tensor(941.0545, grad_fn=<SumBackward0>)\n",
      "log, i 2 tensor(-318.6759, grad_fn=<SumBackward0>) tensor(0., grad_fn=<SumBackward0>)\n",
      "log, i 3 tensor(-941.1310, grad_fn=<SumBackward0>) tensor(0.)\n",
      "log, i 4 tensor(0., grad_fn=<SumBackward0>) tensor(-35.8481, grad_fn=<SumBackward0>)\n",
      "log, i 5 tensor(0.) tensor(217.1418, grad_fn=<SumBackward0>)\n",
      "log, i 6 tensor(35.5407, grad_fn=<SumBackward0>) tensor(0., grad_fn=<SumBackward0>)\n",
      "log, i 7 tensor(-217.0289, grad_fn=<SumBackward0>) tensor(0.)\n",
      "log, i 8 tensor(0., grad_fn=<SumBackward0>) tensor(-56.9320, grad_fn=<SumBackward0>)\n",
      "log, i 9 tensor(0.) tensor(-459.4680, grad_fn=<SumBackward0>)\n",
      "log, i 10 tensor(56.9140, grad_fn=<SumBackward0>) tensor(0., grad_fn=<SumBackward0>)\n",
      "log, i 11 tensor(459.4753, grad_fn=<SumBackward0>) tensor(0.)\n",
      "log, i 12 tensor(0., grad_fn=<SumBackward0>) tensor(-354.8556, grad_fn=<SumBackward0>)\n",
      "log, i 13 tensor(0.) tensor(1509.9612, grad_fn=<SumBackward0>)\n",
      "log, i 14 tensor(354.7503, grad_fn=<SumBackward0>) tensor(0., grad_fn=<SumBackward0>)\n",
      "log, i 15 tensor(-1509.9640, grad_fn=<SumBackward0>) tensor(0.)\n",
      "log, i 16 tensor(0., grad_fn=<SumBackward0>) tensor(454.2853, grad_fn=<SumBackward0>)\n",
      "log, i 17 tensor(0.) tensor(958.8483, grad_fn=<SumBackward0>)\n",
      "log, i 18 tensor(-454.2632, grad_fn=<SumBackward0>) tensor(0., grad_fn=<SumBackward0>)\n",
      "log, i 19 tensor(-958.8596, grad_fn=<SumBackward0>) tensor(0.)\n",
      "log, i 20 tensor(0., grad_fn=<SumBackward0>) tensor(-654.3345, grad_fn=<SumBackward0>)\n",
      "log, i 21 tensor(0.) tensor(-145.8742, grad_fn=<SumBackward0>)\n",
      "log, i 22 tensor(654.3306, grad_fn=<SumBackward0>) tensor(0., grad_fn=<SumBackward0>)\n",
      "log, i 23 tensor(145.8345, grad_fn=<SumBackward0>) tensor(0.)\n",
      "log, i 24 tensor(0., grad_fn=<SumBackward0>) tensor(666.2830, grad_fn=<SumBackward0>)\n",
      "log, i 25 tensor(0.) tensor(-1307.6262, grad_fn=<SumBackward0>)\n",
      "log, i 26 tensor(-666.2821, grad_fn=<SumBackward0>) tensor(0., grad_fn=<SumBackward0>)\n",
      "log, i 27 tensor(1307.6340, grad_fn=<SumBackward0>) tensor(0.)\n",
      "log, i 28 tensor(0., grad_fn=<SumBackward0>) tensor(0.)\n",
      "noise tensor(0., grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'requires_grad'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m data \u001b[38;5;241m=\u001b[39m hist_norm[\u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# data.requires_grad_(True)\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mhist_ld_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires_grad\u001b[49m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m N \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m30\u001b[39m):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# 初始化结果为 logprob\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# log_density = torch.zeros_like(log_prob).requires_grad_(True)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#         print(idx)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m#         log_density += hist_ld_norm[idx].squeeze()  # 用 squeeze() 去除单一维度\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     log_density \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mstack(hist_ld_gen[:\u001b[38;5;241m28\u001b[39m]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'requires_grad'"
     ]
    }
   ],
   "source": [
    "# data = points\n",
    "z_K, ld_gen, OT_cost_gen, hist_gen, hist_ld_gen, z_0 = flow.sample(args.train_batch_size)\n",
    "data = z_K\n",
    "print(data.shape)\n",
    "# data.requires_grad_(True)\n",
    "_, log_prob, _, hist_norm, hist_ld_norm, OT_cost_norm, noise = flow.log_prob(data)\n",
    "# print(\"????????????\", (hist_ld_norm[2]).requires_grad )\n",
    "i = 15\n",
    "print(len(hist_gen), hist_gen[0].shape, len(hist_norm), hist_norm[0].shape)\n",
    "print(len(hist_ld_gen), hist_ld_gen[0].shape, len(hist_ld_norm), hist_ld_norm[0].shape)\n",
    "print('sample', torch.sum(hist_gen[-1]-z_K), torch.sum(hist_gen[0]-z_0))\n",
    "print('inverse', torch.sum(hist_norm[-1]-noise), torch.sum(hist_norm[0]-z_K))\n",
    "for i in range(0, 29, 1):\n",
    "    # print('noise, i',i , torch.sum(hist_gen[i]-hist_norm[28-i]))\n",
    "    # print('log, i',i , torch.sum(hist_ld_gen[28-i]), torch.sum(hist_ld_norm[0+i]))\n",
    "    print('log, i',i , torch.sum(hist_ld_gen[i]), torch.sum(hist_ld_norm[28-i]))\n",
    "    # print(i, torch.mean(torch.sum(torch.stack(hist_ld_gen[-i:]), dim=0) + torch.sum(torch.stack(hist_ld_norm[:i]), dim=0)))\n",
    "print('noise',torch.mean(noise-hist_norm[-1]))\n",
    "\n",
    "# print(log_prob.shape,hist_ld_norm[0].shape)\n",
    "\n",
    "\n",
    "data = hist_norm[4]\n",
    "# data.requires_grad_(True)\n",
    "print(hist_ld_gen.requires_grad)\n",
    "for N in range(1,30):\n",
    "    # 初始化结果为 logprob\n",
    "    # log_density = torch.zeros_like(log_prob).requires_grad_(True)\n",
    "    \n",
    "    # # 累加操作\n",
    "    # for idx in range(N):\n",
    "    #     if hist_ld_norm[idx].requires_grad:\n",
    "    #         print(idx)\n",
    "    #         log_density += hist_ld_norm[idx].squeeze()  # 用 squeeze() 去除单一维度\n",
    "\n",
    "    log_density = torch.mean(torch.sum(torch.stack(hist_ld_gen[:28]), dim=0))\n",
    "    \n",
    "    # log_density = torch.sum(torch.sum(torch.stack(hist_ld_norm[:N]), dim=0))\n",
    "    # # return log_prob + logabsdet, log_prob, logabsdet, hist, hist_ld, OT_cost, noise\n",
    "    # print('????????????', N, torch.sum(hist_ld_norm[N]))\n",
    "    grad = torch.autograd.grad(outputs=log_density, inputs=data, create_graph=True, grad_outputs=torch.ones_like(log_density), allow_unused=True)[0]\n",
    "    if grad is not None:\n",
    "        print('????????????', N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 False\n",
      "1 False\n",
      "2 True\n",
      "Gradient available for index: 2 2 tensor(4096.)\n",
      "3 True\n",
      "Gradient available for index: 3 2 tensor(3361.1338, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 3 3 tensor(4096.)\n",
      "4 True\n",
      "Gradient available for index: 4 2 tensor(3361.1338, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 4 3 tensor(4096., grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 4 4 tensor(4096.)\n",
      "Gradient available for index: 4 5 tensor(4096.)\n",
      "5 True\n",
      "Gradient available for index: 5 2 tensor(3361.1338, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 5 3 tensor(4096., grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 5 4 tensor(4096.)\n",
      "Gradient available for index: 5 5 tensor(4096.)\n",
      "6 True\n",
      "Gradient available for index: 6 2 tensor(3222.2886, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 6 3 tensor(4020.2825, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 6 4 tensor(4020.2825, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 6 5 tensor(4020.2825, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 6 6 tensor(4096.)\n",
      "7 True\n",
      "Gradient available for index: 7 2 tensor(3112.0522, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 7 3 tensor(3925.6123, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 7 4 tensor(3925.6123, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 7 5 tensor(3925.6123, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 7 6 tensor(4029.9194, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 7 7 tensor(4096.)\n",
      "8 True\n",
      "Gradient available for index: 8 2 tensor(3112.0522, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 8 3 tensor(3925.6123, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 8 4 tensor(3925.6123, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 8 5 tensor(3925.6123, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 8 6 tensor(4029.9194, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 8 7 tensor(4096., grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 8 8 tensor(4096.)\n",
      "Gradient available for index: 8 9 tensor(4096.)\n",
      "9 True\n",
      "Gradient available for index: 9 2 tensor(3112.0522, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 9 3 tensor(3925.6123, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 9 4 tensor(3925.6123, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 9 5 tensor(3925.6123, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 9 6 tensor(4029.9194, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 9 7 tensor(4096., grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 9 8 tensor(4096.)\n",
      "Gradient available for index: 9 9 tensor(4096.)\n",
      "10 True\n",
      "Gradient available for index: 10 2 tensor(3596.0752, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 10 3 tensor(4290.2363, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 10 4 tensor(4290.2363, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 10 5 tensor(4290.2363, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 10 6 tensor(4470.0171, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 10 7 tensor(4502.0625, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 10 8 tensor(4502.0625, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 10 9 tensor(4502.0625, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 10 10 tensor(4096.)\n",
      "11 True\n",
      "Gradient available for index: 11 2 tensor(3949.2949, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 11 3 tensor(4776.6436, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 11 4 tensor(4776.6436, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 11 5 tensor(4776.6436, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 11 6 tensor(4996.1240, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 11 7 tensor(5032.8408, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 11 8 tensor(5032.8408, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 11 9 tensor(5032.8408, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 11 10 tensor(4677.1323, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 11 11 tensor(4096.)\n",
      "12 True\n",
      "Gradient available for index: 12 2 tensor(3949.2949, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 3 tensor(4776.6436, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 4 tensor(4776.6436, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 5 tensor(4776.6436, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 6 tensor(4996.1240, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 7 tensor(5032.8408, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 8 tensor(5032.8408, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 9 tensor(5032.8408, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 10 tensor(4677.1323, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 11 tensor(4096., grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 12 tensor(4096.)\n",
      "Gradient available for index: 12 13 tensor(4096.)\n",
      "13 True\n",
      "Gradient available for index: 13 2 tensor(3949.2949, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 13 3 tensor(4776.6436, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 13 4 tensor(4776.6436, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 13 5 tensor(4776.6436, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 13 6 tensor(4996.1240, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 13 7 tensor(5032.8408, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 13 8 tensor(5032.8408, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 13 9 tensor(5032.8408, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 13 10 tensor(4677.1323, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 13 11 tensor(4096., grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 13 12 tensor(4096.)\n",
      "Gradient available for index: 13 13 tensor(4096.)\n",
      "14 True\n",
      "Gradient available for index: 14 2 tensor(4367.2891, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 14 3 tensor(5192.7461, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 14 4 tensor(5192.7461, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 14 5 tensor(5192.7461, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 14 6 tensor(5542.3936, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 14 7 tensor(5825.0874, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 14 8 tensor(5825.0874, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 14 9 tensor(5825.0874, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 14 10 tensor(5355.4526, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 14 11 tensor(4634.6812, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 14 12 tensor(4634.6812, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 14 13 tensor(4634.6812, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 14 14 tensor(4096.)\n",
      "15 True\n",
      "Gradient available for index: 15 2 tensor(3579.3774, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 3 tensor(4043.5305, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 4 tensor(4043.5305, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 5 tensor(4043.5305, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 6 tensor(4394.9629, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 7 tensor(4781.3584, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 8 tensor(4781.3584, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 9 tensor(4781.3584, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 10 tensor(4369.6733, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 11 tensor(3754.7695, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 12 tensor(3754.7695, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 13 tensor(3754.7695, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 14 tensor(3041.1616, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 15 tensor(4096.)\n",
      "16 True\n",
      "Gradient available for index: 16 2 tensor(3579.3774, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 3 tensor(4043.5305, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 4 tensor(4043.5305, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 5 tensor(4043.5305, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 6 tensor(4394.9629, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 7 tensor(4781.3584, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 8 tensor(4781.3584, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 9 tensor(4781.3584, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 10 tensor(4369.6733, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 11 tensor(3754.7695, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 12 tensor(3754.7695, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 13 tensor(3754.7695, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 14 tensor(3041.1616, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 15 tensor(4096., grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 16 tensor(4096.)\n",
      "Gradient available for index: 16 17 tensor(4096.)\n",
      "17 True\n",
      "Gradient available for index: 17 2 tensor(3579.3774, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 17 3 tensor(4043.5305, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 17 4 tensor(4043.5305, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 17 5 tensor(4043.5305, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 17 6 tensor(4394.9629, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 17 7 tensor(4781.3584, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 17 8 tensor(4781.3584, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 17 9 tensor(4781.3584, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 17 10 tensor(4369.6733, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 17 11 tensor(3754.7695, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 17 12 tensor(3754.7695, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 17 13 tensor(3754.7695, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 17 14 tensor(3041.1616, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 17 15 tensor(4096., grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 17 16 tensor(4096.)\n",
      "Gradient available for index: 17 17 tensor(4096.)\n",
      "18 True\n",
      "Gradient available for index: 18 2 tensor(3232.0449, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 18 3 tensor(3657.0815, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 18 4 tensor(3657.0815, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 18 5 tensor(3657.0815, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 18 6 tensor(3995.5813, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 18 7 tensor(4231.2749, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 18 8 tensor(4231.2749, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 18 9 tensor(4231.2749, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 18 10 tensor(3890.5273, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 18 11 tensor(3338.5430, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 18 12 tensor(3338.5430, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 18 13 tensor(3338.5430, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 18 14 tensor(2740.5127, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 18 15 tensor(3691.0381, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 18 16 tensor(3691.0381, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 18 17 tensor(3691.0381, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 18 18 tensor(4096.)\n",
      "19 True\n",
      "Gradient available for index: 19 2 tensor(2785.5596, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 3 tensor(3103.3872, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 4 tensor(3103.3872, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 5 tensor(3103.3872, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 6 tensor(3385.4946, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 7 tensor(3533.5710, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 8 tensor(3533.5710, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 9 tensor(3533.5710, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 10 tensor(3189.0769, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 11 tensor(2742.7651, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 12 tensor(2742.7651, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 13 tensor(2742.7651, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 14 tensor(2275.6182, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 15 tensor(3052.2490, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 16 tensor(3052.2490, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 17 tensor(3052.2490, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 18 tensor(3380.7300, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 19 tensor(4096.)\n",
      "20 True\n",
      "Gradient available for index: 20 2 tensor(2785.5596, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 3 tensor(3103.3872, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 4 tensor(3103.3872, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 5 tensor(3103.3872, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 6 tensor(3385.4946, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 7 tensor(3533.5710, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 8 tensor(3533.5710, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 9 tensor(3533.5710, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 10 tensor(3189.0769, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 11 tensor(2742.7651, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 12 tensor(2742.7651, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 13 tensor(2742.7651, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 14 tensor(2275.6182, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 15 tensor(3052.2490, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 16 tensor(3052.2490, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 17 tensor(3052.2490, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 18 tensor(3380.7300, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 19 tensor(4096., grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 20 tensor(4096.)\n",
      "Gradient available for index: 20 21 tensor(4096.)\n",
      "21 True\n",
      "Gradient available for index: 21 2 tensor(2785.5596, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 21 3 tensor(3103.3872, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 21 4 tensor(3103.3872, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 21 5 tensor(3103.3872, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 21 6 tensor(3385.4946, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 21 7 tensor(3533.5710, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 21 8 tensor(3533.5710, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 21 9 tensor(3533.5710, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 21 10 tensor(3189.0769, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 21 11 tensor(2742.7651, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 21 12 tensor(2742.7651, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 21 13 tensor(2742.7651, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 21 14 tensor(2275.6182, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 21 15 tensor(3052.2490, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 21 16 tensor(3052.2490, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 21 17 tensor(3052.2490, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 21 18 tensor(3380.7300, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 21 19 tensor(4096., grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 21 20 tensor(4096.)\n",
      "Gradient available for index: 21 21 tensor(4096.)\n",
      "22 True\n",
      "Gradient available for index: 22 2 tensor(3512.1848, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 3 tensor(3927.6995, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 4 tensor(3927.6995, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 5 tensor(3927.6995, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 6 tensor(4304.2954, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 7 tensor(4626.5547, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 8 tensor(4626.5547, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 9 tensor(4626.5547, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 10 tensor(4211.8384, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 11 tensor(3598.7358, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 12 tensor(3598.7358, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 13 tensor(3598.7358, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 14 tensor(2899.0234, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 15 tensor(3773.8230, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 16 tensor(3773.8230, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 17 tensor(3773.8230, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 18 tensor(4201.7812, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 19 tensor(5080.4551, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 20 tensor(5080.4551, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 21 tensor(5080.4551, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 22 22 tensor(4096.)\n",
      "23 True\n",
      "Gradient available for index: 23 2 tensor(4253.4453, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 3 tensor(4711.5234, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 4 tensor(4711.5234, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 5 tensor(4711.5234, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 6 tensor(5215.7061, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 7 tensor(5759.7715, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 8 tensor(5759.7715, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 9 tensor(5759.7715, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 10 tensor(4877.1758, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 11 tensor(4151.9790, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 12 tensor(4151.9790, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 13 tensor(4151.9790, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 14 tensor(3265.1611, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 15 tensor(4191.3154, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 16 tensor(4191.3154, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 17 tensor(4191.3154, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 18 tensor(4720.5449, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 19 tensor(5620.2773, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 20 tensor(5620.2773, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 21 tensor(5620.2773, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 22 tensor(4536.8652, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 23 tensor(4096.)\n",
      "24 True\n",
      "Gradient available for index: 24 2 tensor(4253.4453, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 3 tensor(4711.5234, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 4 tensor(4711.5234, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 5 tensor(4711.5234, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 6 tensor(5215.7061, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 7 tensor(5759.7715, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 8 tensor(5759.7715, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 9 tensor(5759.7715, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 10 tensor(4877.1758, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 11 tensor(4151.9790, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 12 tensor(4151.9790, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 13 tensor(4151.9790, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 14 tensor(3265.1611, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 15 tensor(4191.3154, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 16 tensor(4191.3154, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 17 tensor(4191.3154, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 18 tensor(4720.5449, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 19 tensor(5620.2773, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 20 tensor(5620.2773, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 21 tensor(5620.2773, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 22 tensor(4536.8652, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 23 tensor(4096., grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 24 tensor(4096.)\n",
      "Gradient available for index: 24 25 tensor(4096.)\n",
      "25 True\n",
      "Gradient available for index: 25 2 tensor(4253.4453, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 3 tensor(4711.5234, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 4 tensor(4711.5234, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 5 tensor(4711.5234, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 6 tensor(5215.7061, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 7 tensor(5759.7715, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 8 tensor(5759.7715, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 9 tensor(5759.7715, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 10 tensor(4877.1758, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 11 tensor(4151.9790, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 12 tensor(4151.9790, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 13 tensor(4151.9790, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 14 tensor(3265.1611, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 15 tensor(4191.3154, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 16 tensor(4191.3154, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 17 tensor(4191.3154, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 18 tensor(4720.5449, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 19 tensor(5620.2773, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 20 tensor(5620.2773, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 21 tensor(5620.2773, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 22 tensor(4536.8652, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 23 tensor(4096., grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 25 24 tensor(4096.)\n",
      "Gradient available for index: 25 25 tensor(4096.)\n",
      "26 True\n",
      "Gradient available for index: 26 2 tensor(3243.8223, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 3 tensor(3607.8972, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 4 tensor(3607.8972, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 5 tensor(3607.8972, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 6 tensor(3957.9473, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 7 tensor(4241.5459, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 8 tensor(4241.5459, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 9 tensor(4241.5459, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 10 tensor(3570.9456, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 11 tensor(3063.8879, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 12 tensor(3063.8879, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 13 tensor(3063.8879, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 14 tensor(2514.0359, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 15 tensor(3427.3909, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 16 tensor(3427.3909, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 17 tensor(3427.3909, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 18 tensor(3851.0034, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 19 tensor(4629.3965, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 20 tensor(4629.3965, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 21 tensor(4629.3965, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 22 tensor(3769.8350, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 23 tensor(3554.1455, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 24 tensor(3554.1455, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 25 tensor(3554.1455, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 26 26 tensor(4096.)\n",
      "27 True\n",
      "Gradient available for index: 27 2 tensor(3896.6294, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 3 tensor(4453.5815, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 4 tensor(4453.5815, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 5 tensor(4453.5815, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 6 tensor(4862.4248, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 7 tensor(5256.5957, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 8 tensor(5256.5957, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 9 tensor(5256.5957, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 10 tensor(4614.6611, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 11 tensor(3922.2012, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 12 tensor(3922.2012, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 13 tensor(3922.2012, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 14 tensor(3240.9380, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 15 tensor(4652.1650, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 16 tensor(4652.1650, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 17 tensor(4652.1650, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 18 tensor(5246.7974, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 19 tensor(6441.6206, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 20 tensor(6441.6206, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 21 tensor(6441.6206, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 22 tensor(5192.4189, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 23 tensor(5073.5889, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 24 tensor(5073.5889, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 25 tensor(5073.5889, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 26 tensor(5697.2539, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 27 tensor(4096.)\n",
      "28 True\n",
      "Gradient available for index: 28 2 tensor(3896.6294, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 3 tensor(4453.5815, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 4 tensor(4453.5815, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 5 tensor(4453.5815, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 6 tensor(4862.4248, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 7 tensor(5256.5957, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 8 tensor(5256.5957, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 9 tensor(5256.5957, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 10 tensor(4614.6611, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 11 tensor(3922.2012, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 12 tensor(3922.2012, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 13 tensor(3922.2012, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 14 tensor(3240.9380, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 15 tensor(4652.1650, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 16 tensor(4652.1650, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 17 tensor(4652.1650, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 18 tensor(5246.7974, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 19 tensor(6441.6206, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 20 tensor(6441.6206, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 21 tensor(6441.6206, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 22 tensor(5192.4189, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 23 tensor(5073.5889, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 24 tensor(5073.5889, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 25 tensor(5073.5889, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 26 tensor(5697.2539, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 27 tensor(4096., grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 28 tensor(4096.)\n"
     ]
    }
   ],
   "source": [
    "z_K, ld_gen, OT_cost_gen, hist_gen, hist_ld_gen, z_0 = flow.sample(args.train_batch_size)\n",
    "# 假设 hist_ld_norm 是一个包含张量的列表\n",
    "for i, tensor in enumerate(hist_gen):\n",
    "    print(i, tensor.requires_grad)\n",
    "    # 只有当张量需要梯度时才尝试计算梯度\n",
    "    if tensor.requires_grad:\n",
    "        # 确保对应的 hist_norm[i] 也是张量，并且已经设置了 requires_grad\n",
    "        for j in range(29):\n",
    "            if hist_gen[j].requires_grad:  # 检查 hist_gen[j] 是否可以计算梯度\n",
    "                grad = torch.autograd.grad(outputs=tensor, inputs=hist_gen[j], create_graph=True, grad_outputs=torch.ones_like(tensor), allow_unused=True)[0]\n",
    "                if grad is not None:\n",
    "                    print('Gradient available for index:', i, j, torch.sum(grad))\n",
    "    # else:\n",
    "    #     print('Tensor at index', i, 'does not require grad or does not have a grad_fn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1 tensor(0.)\n",
      "1 2 tensor(-1872.8127, grad_fn=<SumBackward0>)\n",
      "2 3 tensor(2225.5869, grad_fn=<SumBackward0>)\n",
      "3 4 tensor(0., grad_fn=<SumBackward0>)\n",
      "4 5 tensor(0., grad_fn=<SumBackward0>)\n",
      "5 6 tensor(-41.2097, grad_fn=<SumBackward0>)\n",
      "6 7 tensor(1384.4688, grad_fn=<SumBackward0>)\n",
      "7 8 tensor(0., grad_fn=<SumBackward0>)\n",
      "8 9 tensor(0., grad_fn=<SumBackward0>)\n",
      "9 10 tensor(4238.3594, grad_fn=<SumBackward0>)\n",
      "10 11 tensor(-1215.6812, grad_fn=<SumBackward0>)\n",
      "11 12 tensor(0., grad_fn=<SumBackward0>)\n",
      "12 13 tensor(0., grad_fn=<SumBackward0>)\n",
      "13 14 tensor(-349.7755, grad_fn=<SumBackward0>)\n",
      "14 15 tensor(560.6802, grad_fn=<SumBackward0>)\n",
      "15 16 tensor(0., grad_fn=<SumBackward0>)\n",
      "16 17 tensor(0., grad_fn=<SumBackward0>)\n",
      "17 18 tensor(-2484.1509, grad_fn=<SumBackward0>)\n",
      "18 19 tensor(382.2468, grad_fn=<SumBackward0>)\n",
      "19 20 tensor(0., grad_fn=<SumBackward0>)\n",
      "20 21 tensor(0., grad_fn=<SumBackward0>)\n",
      "21 22 tensor(-2142.1882, grad_fn=<SumBackward0>)\n",
      "22 23 tensor(-850.0787, grad_fn=<SumBackward0>)\n",
      "23 24 tensor(0., grad_fn=<SumBackward0>)\n",
      "24 25 tensor(0., grad_fn=<SumBackward0>)\n",
      "25 26 tensor(-573.3685, grad_fn=<SumBackward0>)\n",
      "26 27 tensor(-932.9075, grad_fn=<SumBackward0>)\n",
      "27 28 tensor(0., grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z_K, ld_gen, OT_cost_gen, hist_gen, hist_ld_gen, z_0 = flow.sample(args.train_batch_size)\n",
    "for i in range(28):\n",
    "    print(i, i+1, torch.sum(hist_gen[i]-hist_gen[i+1].squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "0 1 tensor(0., grad_fn=<SumBackward0>)\n",
      "1 2 tensor(0., grad_fn=<SumBackward0>)\n",
      "2 3 tensor(466.9605, grad_fn=<SumBackward0>)\n",
      "3 4 tensor(741.4856, grad_fn=<SumBackward0>)\n",
      "4 5 tensor(0., grad_fn=<SumBackward0>)\n",
      "5 6 tensor(0., grad_fn=<SumBackward0>)\n",
      "6 7 tensor(822.1868, grad_fn=<SumBackward0>)\n",
      "7 8 tensor(1926.5005, grad_fn=<SumBackward0>)\n",
      "8 9 tensor(0., grad_fn=<SumBackward0>)\n",
      "9 10 tensor(0., grad_fn=<SumBackward0>)\n",
      "10 11 tensor(-173.1436, grad_fn=<SumBackward0>)\n",
      "11 12 tensor(2578.5776, grad_fn=<SumBackward0>)\n",
      "12 13 tensor(0., grad_fn=<SumBackward0>)\n",
      "13 14 tensor(0., grad_fn=<SumBackward0>)\n",
      "14 15 tensor(216.7531, grad_fn=<SumBackward0>)\n",
      "15 16 tensor(410.6828, grad_fn=<SumBackward0>)\n",
      "16 17 tensor(0., grad_fn=<SumBackward0>)\n",
      "17 18 tensor(0., grad_fn=<SumBackward0>)\n",
      "18 19 tensor(985.2815, grad_fn=<SumBackward0>)\n",
      "19 20 tensor(-3719.9143, grad_fn=<SumBackward0>)\n",
      "20 21 tensor(0., grad_fn=<SumBackward0>)\n",
      "21 22 tensor(0., grad_fn=<SumBackward0>)\n",
      "22 23 tensor(-1192.4768, grad_fn=<SumBackward0>)\n",
      "23 24 tensor(-35.8278, grad_fn=<SumBackward0>)\n",
      "24 25 tensor(0., grad_fn=<SumBackward0>)\n",
      "25 26 tensor(0., grad_fn=<SumBackward0>)\n",
      "26 27 tensor(-1873.9609, grad_fn=<SumBackward0>)\n",
      "27 28 tensor(1907.7173, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "_, log_prob, _, hist_norm, hist_ld_norm, OT_cost_norm, noise = flow.log_prob(data)\n",
    "print(len(hist_ld_norm))\n",
    "for i in range(28):\n",
    "    print(i, i+1, torch.sum(hist_norm[i]-hist_norm[i+1].squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 False\n",
      "1 False\n",
      "2 True\n",
      "3 True\n",
      "Gradient available for index: 3 0 tensor(-654.1703, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 3 1 tensor(-654.1703, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 3 2 tensor(-654.1703, grad_fn=<SumBackward0>)\n",
      "4 True\n",
      "Gradient available for index: 4 0 tensor(-349.1279, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 4 1 tensor(-349.1279, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 4 2 tensor(-349.1279, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 4 3 tensor(-1559.9524, grad_fn=<SumBackward0>)\n",
      "5 False\n",
      "6 True\n",
      "7 True\n",
      "Gradient available for index: 7 0 tensor(647.9644, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 7 1 tensor(647.9644, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 7 2 tensor(647.9644, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 7 3 tensor(1392.3075, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 7 4 tensor(3165.7761, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 7 5 tensor(3165.7761, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 7 6 tensor(3165.7761, grad_fn=<SumBackward0>)\n",
      "8 True\n",
      "Gradient available for index: 8 0 tensor(-1165.6278, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 8 1 tensor(-1165.6278, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 8 2 tensor(-1165.6278, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 8 3 tensor(-979.2766, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 8 4 tensor(-664.9354, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 8 5 tensor(-664.9354, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 8 6 tensor(-664.9354, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 8 7 tensor(-413.8310, grad_fn=<SumBackward0>)\n",
      "9 False\n",
      "10 True\n",
      "11 True\n",
      "Gradient available for index: 11 0 tensor(1109.7744, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 11 1 tensor(1109.7744, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 11 2 tensor(1109.7744, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 11 3 tensor(1053.4728, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 11 4 tensor(1049.8260, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 11 5 tensor(1049.8260, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 11 6 tensor(1049.8260, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 11 7 tensor(1005.8443, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 11 8 tensor(1086.8113, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 11 9 tensor(1086.8113, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 11 10 tensor(1086.8113, grad_fn=<SumBackward0>)\n",
      "12 True\n",
      "Gradient available for index: 12 0 tensor(3817.2905, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 1 tensor(3817.2905, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 2 tensor(3817.2905, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 3 tensor(2617.3354, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 4 tensor(1165.3960, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 5 tensor(1165.3960, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 6 tensor(1165.3960, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 7 tensor(637.1937, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 8 tensor(607.4283, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 9 tensor(607.4283, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 10 tensor(607.4283, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 12 11 tensor(765.8976, grad_fn=<SumBackward0>)\n",
      "13 False\n",
      "14 True\n",
      "15 True\n",
      "Gradient available for index: 15 0 tensor(503.5852, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 1 tensor(503.5852, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 2 tensor(503.5852, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 3 tensor(1563.5751, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 4 tensor(2245.6604, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 5 tensor(2245.6604, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 6 tensor(2245.6604, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 7 tensor(3081.1677, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 8 tensor(3742.2988, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 9 tensor(3742.2988, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 10 tensor(3742.2988, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 11 tensor(2964.6992, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 12 tensor(2073.6250, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 13 tensor(2073.6250, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 15 14 tensor(2073.6250, grad_fn=<SumBackward0>)\n",
      "16 True\n",
      "Gradient available for index: 16 0 tensor(330.6634, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 1 tensor(330.6634, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 2 tensor(330.6634, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 3 tensor(1631.2352, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 4 tensor(3042.7019, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 5 tensor(3042.7019, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 6 tensor(3042.7019, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 7 tensor(4157.8838, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 8 tensor(4756.6938, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 9 tensor(4756.6938, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 10 tensor(4756.6938, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 11 tensor(3773.6436, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 12 tensor(2606.0303, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 13 tensor(2606.0303, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 14 tensor(2606.0303, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 16 15 tensor(700.0540, grad_fn=<SumBackward0>)\n",
      "17 False\n",
      "18 True\n",
      "19 True\n",
      "Gradient available for index: 19 0 tensor(-192.5717, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 1 tensor(-192.5717, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 2 tensor(-192.5717, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 3 tensor(664.4718, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 4 tensor(1788.7571, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 5 tensor(1788.7571, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 6 tensor(1788.7571, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 7 tensor(2511.6934, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 8 tensor(2828.8462, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 9 tensor(2828.8462, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 10 tensor(2828.8462, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 11 tensor(2221.7971, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 12 tensor(1657.1127, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 13 tensor(1657.1127, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 14 tensor(1657.1127, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 15 tensor(353.1221, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 16 tensor(158.4737, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 17 tensor(158.4737, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 19 18 tensor(158.4737, grad_fn=<SumBackward0>)\n",
      "20 True\n",
      "Gradient available for index: 20 0 tensor(-1141.3501, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 1 tensor(-1141.3501, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 2 tensor(-1141.3501, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 3 tensor(-1322.4700, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 4 tensor(102.5276, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 5 tensor(102.5276, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 6 tensor(102.5276, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 7 tensor(29.0079, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 8 tensor(-811.9633, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 9 tensor(-811.9633, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 10 tensor(-811.9633, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 11 tensor(-670.8020, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 12 tensor(756.9319, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 13 tensor(756.9319, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 14 tensor(756.9319, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 15 tensor(67.2601, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 16 tensor(-790.0291, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 17 tensor(-790.0291, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 18 tensor(-790.0291, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 20 19 tensor(-1675.5955, grad_fn=<SumBackward0>)\n",
      "21 False\n",
      "22 True\n",
      "23 True\n",
      "Gradient available for index: 23 0 tensor(-467.2540, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 1 tensor(-467.2540, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 2 tensor(-467.2540, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 3 tensor(794.0543, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 4 tensor(1929.9869, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 5 tensor(1929.9869, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 6 tensor(1929.9869, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 7 tensor(2505.0542, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 8 tensor(2993.7954, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 9 tensor(2993.7954, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 10 tensor(2993.7954, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 11 tensor(2092.7878, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 12 tensor(781.9927, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 13 tensor(781.9927, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 14 tensor(781.9927, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 15 tensor(-529.2812, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 16 tensor(-787.4803, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 17 tensor(-787.4803, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 18 tensor(-787.4803, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 19 tensor(-156.4897, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 20 tensor(350.3428, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 21 tensor(350.3428, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 23 22 tensor(350.3428, grad_fn=<SumBackward0>)\n",
      "24 True\n",
      "Gradient available for index: 24 0 tensor(-1220.8861, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 1 tensor(-1220.8861, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 2 tensor(-1220.8861, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 3 tensor(-1145.1482, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 4 tensor(-1537.9446, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 5 tensor(-1537.9446, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 6 tensor(-1537.9446, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 7 tensor(-1702.0830, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 8 tensor(-1736.8579, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 9 tensor(-1736.8579, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 10 tensor(-1736.8579, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 11 tensor(-1574.1428, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 12 tensor(-2397.7363, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 13 tensor(-2397.7363, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 14 tensor(-2397.7363, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 15 tensor(-1835.6201, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 16 tensor(-1840.0283, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 17 tensor(-1840.0283, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 18 tensor(-1840.0283, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 19 tensor(-1324.5818, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 20 tensor(-770.2683, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 21 tensor(-770.2683, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 22 tensor(-770.2683, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 24 23 tensor(-792.3503, grad_fn=<SumBackward0>)\n",
      "25 False\n",
      "26 True\n",
      "27 True\n",
      "Gradient available for index: 27 0 tensor(70.8839, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 1 tensor(70.8839, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 2 tensor(70.8839, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 3 tensor(580.0688, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 4 tensor(993.2488, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 5 tensor(993.2488, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 6 tensor(993.2488, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 7 tensor(1113.1376, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 8 tensor(1326.8525, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 9 tensor(1326.8525, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 10 tensor(1326.8525, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 11 tensor(913.7085, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 12 tensor(311.5376, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 13 tensor(311.5376, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 14 tensor(311.5376, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 15 tensor(-250.1785, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 16 tensor(-373.6193, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 17 tensor(-373.6193, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 18 tensor(-373.6193, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 19 tensor(-173.1628, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 20 tensor(-137.4722, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 21 tensor(-137.4722, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 22 tensor(-137.4722, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 23 tensor(-41.5190, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 24 tensor(-138.9256, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 25 tensor(-138.9256, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 27 26 tensor(-138.9256, grad_fn=<SumBackward0>)\n",
      "28 True\n",
      "Gradient available for index: 28 0 tensor(3757.8591, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 1 tensor(3757.8591, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 2 tensor(3757.8591, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 3 tensor(4483.5923, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 4 tensor(4663.9048, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 5 tensor(4663.9048, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 6 tensor(4663.9048, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 7 tensor(5277.4189, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 8 tensor(6004.6953, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 9 tensor(6004.6953, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 10 tensor(6004.6953, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 11 tensor(4775.1221, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 12 tensor(4313.5146, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 13 tensor(4313.5146, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 14 tensor(4313.5146, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 15 tensor(2872.3188, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 16 tensor(3307.7454, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 17 tensor(3307.7454, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 18 tensor(3307.7454, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 19 tensor(3093.8994, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 20 tensor(2200.6191, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 21 tensor(2200.6191, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 22 tensor(2200.6191, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 23 tensor(2096.0198, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 24 tensor(1826.3838, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 25 tensor(1826.3838, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 26 tensor(1826.3838, grad_fn=<SumBackward0>)\n",
      "Gradient available for index: 28 27 tensor(1849.2200, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "_, log_prob, _, hist_norm, hist_ld_norm, OT_cost_norm, noise = flow.log_prob(data)\n",
    "# 假设 hist_ld_norm 是一个包含张量的列表\n",
    "for i, tensor in enumerate(hist_ld_norm):\n",
    "    # tensor = log_prob\n",
    "    print(i, tensor.requires_grad)\n",
    "    # 只有当张量需要梯度时才尝试计算梯度\n",
    "    if tensor.requires_grad:\n",
    "        # 确保对应的 hist_norm[i] 也是张量，并且已经设置了 requires_grad\n",
    "        for j in range(29):\n",
    "            grad = torch.autograd.grad(outputs=tensor, inputs=hist_norm[j], create_graph=True, grad_outputs=torch.ones_like(tensor), allow_unused=True)[0]\n",
    "            if grad is not None:\n",
    "                print('Gradient available for index:', i, j, torch.sum(grad))\n",
    "    # else:\n",
    "    #     print('Tensor at index', i, 'does not require grad or does not have a grad_fn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "log_density = torch.mean(torch.sum(torch.stack(hist_ld_norm[:28]), dim=0))\n",
    "print(log_density.requires_grad)\n",
    "# log_density = hist_ld_norm[24]\n",
    "data = hist_norm[4]\n",
    "# data.requires_grad_(True)\n",
    "for N in range(0,29):\n",
    "    data = hist_norm[i]\n",
    "    data.requires_grad_(True)  # 确保data可以计算梯度\n",
    "    # log_density = torch.mean(torch.sum(torch.stack(hist_ld_gen[:28]), dim=0))\n",
    "    grad = torch.autograd.grad(outputs=log_density, inputs=data, create_graph=True, grad_outputs=torch.ones_like(log_density), allow_unused=True)[0]\n",
    "    if grad is not None:\n",
    "        print('????????????', N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m samples, log_prob, logabsdet,  hist, hist_ld, noise \u001b[38;5;241m=\u001b[39m flow\u001b[38;5;241m.\u001b[39msample1(args\u001b[38;5;241m.\u001b[39mtrain_batch_size)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# 计算 log_prob 相对于 noise 的梯度\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m grad \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# 输出梯度\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(grad)\n",
      "File \u001b[0;32m~/anaconda3/envs/env1/lib/python3.11/site-packages/torch/autograd/__init__.py:394\u001b[0m, in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[1;32m    390\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[1;32m    391\u001b[0m         grad_outputs_\n\u001b[1;32m    392\u001b[0m     )\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 394\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[1;32m    404\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    405\u001b[0m         output\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mzeros_like(\u001b[38;5;28minput\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m (output, \u001b[38;5;28minput\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(result, t_inputs)\n\u001b[1;32m    409\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "samples, log_prob, logabsdet,  hist, hist_ld, noise = flow.sample1(args.train_batch_size)\n",
    "# 计算 log_prob 相对于 noise 的梯度\n",
    "grad = torch.autograd.grad(outputs=log_prob, inputs=noise, create_graph=True, grad_outputs=torch.ones_like(log_prob), allow_unused=True)[0]\n",
    "\n",
    "# 输出梯度\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  4,  8, 12, 16, 20, 24, 28])\n"
     ]
    }
   ],
   "source": [
    "torch.sum(torch.stack(hist_ld_norm[:i]), dim=0).shape\n",
    "\n",
    "# Running cost\n",
    "hist   = hist_gen\n",
    "part = torch.cat(hist).reshape(len(hist), hist[0].shape[0], hist[0].shape[-1]).permute(1,0,2) # B x K x d\n",
    "K    = part.shape[1]\n",
    "I = torch.arange(0, K, step=4)\n",
    "print(I)\n",
    "part = part[:,I,:]\n",
    "K = part.shape[1]-1\n",
    "I = torch.arange(K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/10 [00:00<?, ?iter/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The value argument to log_prob must be a Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 113\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# grab data\u001b[39;00m\n\u001b[1;32m    112\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(train_generator)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m--> 113\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# 保存loss值\u001b[39;00m\n\u001b[1;32m    115\u001b[0m losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[77], line 46\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(flow, args, data, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m KL_density  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m KL_sampling \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 46\u001b[0m log_prob_1  \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(\u001b[43mtarget_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     47\u001b[0m log_prob_0   \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(distribution\u001b[38;5;241m.\u001b[39mlog_prob(z_0))\n\u001b[1;32m     48\u001b[0m log_prob_gen \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(target_dist\u001b[38;5;241m.\u001b[39mlog_prob(z_K))\n",
      "File \u001b[0;32m~/anaconda3/envs/env1/lib/python3.11/site-packages/torch/distributions/mixture_same_family.py:161\u001b[0m, in \u001b[0;36mMixtureSameFamily.log_prob\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlog_prob\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_args:\n\u001b[0;32m--> 161\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pad(x)\n\u001b[1;32m    163\u001b[0m     log_prob_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponent_distribution\u001b[38;5;241m.\u001b[39mlog_prob(x)  \u001b[38;5;66;03m# [S, B, k]\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/env1/lib/python3.11/site-packages/torch/distributions/distribution.py:285\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;03mArgument validation for distribution methods such as `log_prob`,\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;03m`cdf` and `icdf`. The rightmost dimensions of a value to be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m        distribution's batch and event shapes.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe value argument to log_prob must be a Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    287\u001b[0m event_dim_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(value\u001b[38;5;241m.\u001b[39msize()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_shape)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m value\u001b[38;5;241m.\u001b[39msize()[event_dim_start:] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_shape:\n",
      "\u001b[0;31mValueError\u001b[0m: The value argument to log_prob must be a Tensor"
     ]
    }
   ],
   "source": [
    "# =================================================================================== #\n",
    "#                                      Training                                       #\n",
    "# =================================================================================== #\n",
    "beta = 0.5\n",
    "def Energy(x, logp):\n",
    "    energy = 0\n",
    "\n",
    "    potential = (x[:,0] - 1)**2 + (x[:,1] - 1)**2\n",
    "    energy = torch.mean(potential) + 1/beta * torch.mean(logp)\n",
    "    return energy\n",
    "\n",
    "def GradF2(x, logp):\n",
    "    x = torch.squeeze(x)\n",
    "    N = x.shape[0]\n",
    "    energy = Energy(x, logp) * N\n",
    "    grad = torch.autograd.grad(outputs=energy, inputs=x, create_graph=True, grad_outputs=torch.ones_like(energy))[0]\n",
    "    gradF2 = (torch.sum(grad ** 2) * 1/N)\n",
    "    return gradF2\n",
    "\n",
    "def compute_loss(flow,  args, data=None, mode='train'):\n",
    "    # forward\n",
    "    # log_density, _, _, hist_norm, hist_ld_norm, OT_cost_norm, _ = flow.log_prob(data)\n",
    "    # # Distribution.log_prob, Flow._log_prob, Flow._transform(inputs, context=context), CouplingTransform.forward\n",
    "    # # return log_prob + logabsdet, log_prob, logabsdet, hist, hist_ld, OT_cost, noise\n",
    "    # log_density = torch.mean(log_density)\n",
    "\n",
    "    # sample if needed\n",
    "    # z_K, logp0, logdet, hist_gen, hist_ld_gen, z_0 = flow.sample1(args.train_batch_size)\n",
    "    # samples, log_prob, logabsdet,  hist, hist_ld, noise\n",
    "    # grad = torch.autograd.grad(outputs=logp0-logdet, inputs=z_K, create_graph=True, grad_outputs=torch.ones_like(ld_gen))[0]\n",
    "    # print('????????????',grad)\n",
    "    # summed_tensor = torch.sum(torch.stack(hist_ld_gen), dim=0)\n",
    "    # print('print(torch.mean(summed_tensor-ld_gen))',torch.mean(summed_tensor-ld_gen))\n",
    "    # print('print(torch.sum(hist_ld_gen[0]))',torch.sum(hist_ld_gen[0]))\n",
    "    # print('print(len(hist_ld_gen))',len(hist_ld_gen), hist_ld_gen[0].shape)\n",
    "    # Distribution.sample, Flow._sample, Flow._transform.inverse, CouplingTransform.inverse\n",
    "    # distribution matching / terminal cost (G)\n",
    "\n",
    "    z_K, ld_gen, OT_cost_gen, hist_gen, hist_ld_gen, z_0 = flow.sample(args.train_batch_size)\n",
    "\n",
    "\n",
    "    \n",
    "    KL_density  = torch.Tensor([0]).to(device)\n",
    "    KL_sampling = torch.Tensor([0]).to(device)\n",
    "    \n",
    "    log_prob_1  = torch.mean(target_dist.log_prob(data))\n",
    "    log_prob_0   = torch.mean(distribution.log_prob(z_0))\n",
    "    log_prob_gen = torch.mean(target_dist.log_prob(z_K))\n",
    "    ld_gen       = torch.mean(ld_gen)\n",
    "    KL_density   = - (log_prob_gen + ld_gen - log_prob_0)\n",
    "    G_cost       = log_prob_1 - log_density + KL_density\n",
    "    G_cost =  Energy(z_K, log_density) - log_density\n",
    "\n",
    "    # OT regularization (L)\n",
    "    L_cost = torch.Tensor([0]).to(device)\n",
    "\n",
    "\n",
    "    # sample a batch from the base to compute the OT cost\n",
    "    \n",
    "    # Running cost\n",
    "    hist   = hist_gen\n",
    "    part = torch.cat(hist).reshape(len(hist), hist[0].shape[0], hist[0].shape[-1]).permute(1,0,2) # B x K x d\n",
    "    K    = part.shape[1]\n",
    "    I = torch.arange(0, K, step=4)\n",
    "    print(I)\n",
    "    part = part[:,I,:]\n",
    "    K = part.shape[1]-1\n",
    "    I = torch.arange(K)\n",
    "    \n",
    "    Energy_seq = torch.zeros(K+1)\n",
    "    Energy_seq[0]=Energy(part[:,0,:], logp0)   \n",
    "    arclength_seq = torch.zeros(K)\n",
    "    action_seq = torch.zeros(K)\n",
    "    cosvalue = torch.zeros(K)\n",
    "    running_loss = 0\n",
    "    for i in range(1,K+1):\n",
    "        # print(i)\n",
    "        # print(part.shape,part[:,i,:].shape)\n",
    "        Energy_seq[i] = Energy(part[:,i,:])\n",
    "        arclength2 = torch.mean(torch.norm(part[:,i,:] - part[:,i-1,:], dim=[1])**2)\n",
    "        arclength_seq[i-1] =arclength2\n",
    "        gradF2 = GradF2(part[:,i,:])\n",
    "        running_loss += torch.sqrt(gradF2 * arclength2)\n",
    "        action_seq[i-1] = torch.sqrt(gradF2 * arclength2)\n",
    "        cosvalue[i-1] = torch.sqrt((Energy_seq[i]-Energy_seq[i-1])**2 / (gradF2 * arclength2) )\n",
    "    L = torch.mean(K * torch.norm(part[:,I,:] - part[:,I+1,:], dim=[1,2])**2)\n",
    "    differences = Energy_seq[1:] - Energy_seq[:-1]\n",
    "    energydiff_loss = (torch.max(differences)/torch.min(differences)-1)**2\n",
    "    arclendiff_loss = (torch.max(arclength_seq)/torch.min(arclength_seq)-1)**2\n",
    "    arcactdiff_loss = (torch.max(action_seq)/torch.min(action_seq)-1)**2\n",
    "    # interaction (F)\n",
    "    F_P    = torch.Tensor([0]).to(device)\n",
    "    F_E    = torch.Tensor([0]).to(device)\n",
    "    F_cost = torch.Tensor([0]).to(device)\n",
    "    \n",
    "    # Overall loss\n",
    "    loss = G_cost + args.lbd_OT * (L)\n",
    "\n",
    "    # return loss, G_cost, L_cost, F_cost\n",
    "    return {'loss': loss, 'Terminal': G_cost, 'Running_OT': L, 'Running_action':running_loss, 'arc': (arcactdiff_loss), 'F': F_cost, 'Energy_seq': Energy_seq, 'arclength_seq': arclength_seq, 'action_seq': action_seq, 'cosvalue': cosvalue}\n",
    "\n",
    "# main loop\n",
    "# 初始化存储损失值的字典\n",
    "losses = {'loss': [], 'Terminal': [], 'Running_OT': [], 'Running_action': [], 'arc': []}\n",
    "\n",
    "args.num_training_steps = 10\n",
    "for step in tqdm(range(args.num_training_steps), desc='Training Progress', unit='iter'):\n",
    "    flow.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # grab data\n",
    "    batch = next(train_generator).to(device)\n",
    "    loss_dict = compute_loss(flow, args, mode='train')\n",
    "    # 保存loss值\n",
    "    losses['loss'].append(loss_dict['loss'].item())\n",
    "    losses['Terminal'].append(loss_dict['Terminal'].item())\n",
    "    losses['Running_OT'].append(loss_dict['Running_OT'].item())\n",
    "    losses['Running_action'].append(loss_dict['Running_action'].item())\n",
    "    losses['arc'].append(loss_dict['arc'].item())\n",
    "\n",
    "\n",
    "    loss      = loss_dict['loss']\n",
    "\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(flow.parameters(), args.grad_norm_clip_value)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# 画图部分\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "loss_types = ['loss', 'Terminal', 'Running_OT', 'Running_action','arc']\n",
    "titles = ['loss', 'Terminal', 'Running_OT', 'Running_action','arc']\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.plot(losses[loss_types[i]], label=loss_types[i])\n",
    "    ax.set_title(titles[i])\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "\n",
    "    # 标注最后一点的值\n",
    "    final_value = losses[loss_types[i]][-1]\n",
    "    ax.annotate(f'{final_value:.2f}', # 格式化为两位小数\n",
    "                xy=(len(losses[loss_types[i]]) - 1, final_value),\n",
    "                xytext=(8, 0), \n",
    "                textcoords='offset points',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('running_loss',losses['Running_action'][-1])\n",
    "\n",
    "loss_dict = compute_loss(flow, points, args, mode='train')\n",
    "loss_types = ['Energy_seq', 'arclength_seq', 'action_seq', 'cosvalue']\n",
    "titles = ['Energy Sequence', 'Arclength Sequence', 'Action Sequence', 'cosvalue']\n",
    "\n",
    "# 创建4个子图\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "# 遍历每个子图并绘制相应的loss数据\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.plot(loss_dict[loss_types[i]].data.cpu().numpy(), marker='o')\n",
    "    ax.set_title(titles[i])\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('energy difference', loss_dict['Energy_seq'].max()-loss_dict['Energy_seq'].min())\n",
    "\n",
    "z_K, ld_gen, OT_cost_gen, hist_gen, hist_ld_gen, z_0 = flow.sample(args.train_batch_size)\n",
    "hist = hist_gen\n",
    "part = torch.cat(hist).reshape(len(hist), hist[0].shape[0], hist[0].shape[-1]).permute(1,0,2)\n",
    "K = part.shape[1]\n",
    "I = torch.arange(0, K, step=4)\n",
    "part = part[:,I,:]\n",
    "\n",
    "main_plot(part, z_0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
