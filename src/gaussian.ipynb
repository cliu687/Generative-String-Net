{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import os, sys, shutil\n",
    "\n",
    "from tensorboardX import SummaryWriter\n",
    "from time import sleep\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "import torch.distributions as D\n",
    "\n",
    "import data as data_\n",
    "import nn as nn_\n",
    "import utils\n",
    "from mfp_utils import *\n",
    "\n",
    "from experiments import cutils\n",
    "from nde import distributions, flows, transforms\n",
    "\n",
    "import klampt\n",
    "from klampt.plan import cspace, robotplanning\n",
    "from klampt.plan.robotcspace import RobotCSpace\n",
    "from klampt.model import collide\n",
    "from klampt.model.trajectory import RobotTrajectory\n",
    "from klampt.io import resource\n",
    "\n",
    "from mfg_models import construct_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "parser = argparse.ArgumentParser()\n",
    "# 模拟命令行输入\n",
    "sys.argv = ['--exp_name 2D --linear_transform_type lu_no_perm --reg_OT_dir gen --num_training_steps 1000 --base_transform_type rq-coupling --tail_bound 5 --OT_part block_CL_no_perm --lbd_OT 2e-1 --gaussian_multi_dim 2 --num_train_data 2000 --LU_last false --NF_loss jeffery --lr_schedule adaptive --train_batch_size 2048 --learning_rate 1e-3']\n",
    "# data\n",
    "parser.add_argument('--exp_name', type=str, default='2D')\n",
    "parser.add_argument('--dataset_name', type=str, default='gaussian_mixture',\n",
    "                    choices=['gaussian_mixture', 'crowd_motion_gaussian', 'crowd_motion_gaussian_close'\n",
    "                            'crowd_motion_gaussian_nonsmooth_obs', 'moons', \n",
    "                            'gaussian', '2spirals', 'checkerboard',\n",
    "                            'power', 'gas', 'hepmass', 'miniboone', 'bsds300',\n",
    "                            'robot_1'],\n",
    "                    help='Name of dataset to use.')\n",
    "parser.add_argument('--val_frac', type=float, default=1.,\n",
    "                    help='Fraction of validation set to use.')\n",
    "\n",
    "# optimization\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-3,\n",
    "                    help='Learning rate for optimizer.')\n",
    "parser.add_argument('--num_training_steps', type=int, default=1000,\n",
    "                    help='Number of total training steps.')\n",
    "# parser.add_argument('--anneal_learning_rate', type=int, default=1,\n",
    "#                     choices=[0, 1],\n",
    "#                     help='Whether to anneal the learning rate.')\n",
    "parser.add_argument('--lr_schedule', type=str, default='adaptive',\n",
    "                    choices=['none', 'cyclic', 'adaptive'])\n",
    "parser.add_argument('--grad_norm_clip_value', type=float, default=5.,\n",
    "                    help='Value by which to clip norm of gradients.')\n",
    "parser.add_argument('--lbd_reg', type=float, default=0)\n",
    "\n",
    "\n",
    "# flow details\n",
    "parser.add_argument('--base_transform_type', type=str, default='rq-coupling',\n",
    "                    choices=['affine-coupling', 'quadratic-coupling', 'rq-coupling',\n",
    "                             'affine-autoregressive', 'quadratic-autoregressive',\n",
    "                             'rq-autoregressive'],\n",
    "                    help='Type of transform to use between linear layers.')\n",
    "parser.add_argument('--linear_transform_type', type=str, default='lu_no_perm',\n",
    "                    choices=['permutation', 'lu', 'svd', 'lu_no_perm'],\n",
    "                    help='Type of linear transform to use.')\n",
    "parser.add_argument('--num_flow_steps', type=int, default=7,\n",
    "                    help='Number of blocks to use in flow.')\n",
    "parser.add_argument('--hidden_features', type=int, default=256,\n",
    "                    help='Number of hidden features to use in coupling/autoregressive nets.')\n",
    "parser.add_argument('--tail_bound', type=float, default=5,\n",
    "                    help='Box is on [-bound, bound]^2')\n",
    "parser.add_argument('--num_bins', type=int, default=8,\n",
    "                    help='Number of bins to use for piecewise transforms.')\n",
    "parser.add_argument('--num_transform_blocks', type=int, default=2,\n",
    "                    help='Number of blocks to use in coupling/autoregressive nets.')\n",
    "parser.add_argument('--use_batch_norm', type=int, default=0,\n",
    "                    choices=[0, 1],\n",
    "                    help='Whether to use batch norm in coupling/autoregressive nets.')\n",
    "parser.add_argument('--dropout_probability', type=float, default=0.25,\n",
    "                    help='Dropout probability for coupling/autoregressive nets.')\n",
    "parser.add_argument('--apply_unconditional_transform', type=int, default=1,\n",
    "                    choices=[0, 1],\n",
    "                    help='Whether to unconditionally transform \\'identity\\' '\n",
    "                         'features in coupling layer.')\n",
    "parser.add_argument('--base_net_act', type=str, default='relu',\n",
    "                    choices=['relu', 'tanh'])\n",
    "\n",
    "# logging and checkpoints\n",
    "parser.add_argument('--monitor_interval', type=int, default=250,\n",
    "                    help='Interval in steps at which to report training stats.')\n",
    "\n",
    "# reproducibility\n",
    "parser.add_argument('--seed', type=int, default=1638128,\n",
    "                    help='Random seed for PyTorch and NumPy.')\n",
    "\n",
    "# MFG\n",
    "parser.add_argument('--gaussian_multi_dim', type=int, default=2)\n",
    "parser.add_argument('--gaussian_multi_a',   type=float, default=10.)\n",
    "parser.add_argument('--num_train_data',     type=int, default=2000)\n",
    "parser.add_argument('--num_val_data',       type=int, default=10000)\n",
    "parser.add_argument('--num_test_data',      type=int, default=10000)\n",
    "parser.add_argument('--train_batch_size',   type=int, default=2048)\n",
    "parser.add_argument('--val_batch_size',     type=int, default=512)\n",
    "parser.add_argument('--test_batch_size',    type=int, default=512)\n",
    "parser.add_argument('--lbd_OT',             type=float, default=2e-1)\n",
    "parser.add_argument('--lbd_F',              type=float, default=0)\n",
    "parser.add_argument('--lbd_F_E',            type=float, default=0.01)\n",
    "parser.add_argument('--lbd_F_P',            type=float, default=1)\n",
    "parser.add_argument('--reg_OT_dir',         type=str, default='gen', choices=['gen', 'norm'])\n",
    "parser.add_argument('--OT_comp',            type=str, default='trajectory', choices=['trajectory', 'monge'])\n",
    "parser.add_argument('--OT_part',            type=str, default='block_CL_no_perm', choices=['block', 'block_CL_no_perm', 'module'])\n",
    "parser.add_argument('--interaction',        type=lambda x: (str(x).lower() == 'true'), default=False)\n",
    "parser.add_argument('--LU_last',            type=lambda x: (str(x).lower() == 'true'), default=False)\n",
    "parser.add_argument('--NF_loss',            type=str, default='jeffery', choices=[\n",
    "                                            'KL_sampling', 'KL_density', 'jeffery'])\n",
    "parser.add_argument('--val_score',          type=str, default='loss', choices=[\n",
    "                                            'loss', 'L', 'G', 'F'])\n",
    "parser.add_argument('--mixture_base',       type=str, default='gaussian', choices=[\n",
    "                                            'gaussian', 'gaussian_mixture'])\n",
    "parser.add_argument('--mixture_weight',     type=str, default='identical', choices=[\n",
    "                                            'identical', 'undersample_one'])  \n",
    "parser.add_argument('--F_ld_weight',        type=str, default='identical', choices=['identical'])                                                                          \n",
    "parser.add_argument('--disc_scheme',        type=str, default='forward', choices=[\n",
    "                                            'forward', 'centered', 'forward_2nd',\n",
    "                                            'FD4_simp', 'FD1_simp', 'FD4_simp_symmetric'])\n",
    "parser.add_argument('--NF_model',           type=str, default='default', choices=[\n",
    "                                            'default', 'single_flow'])                                     \n",
    "parser.add_argument('--obs_nonsmooth_val',  type=float, default=100.)\n",
    "parser.add_argument('--interp_hist',        type=lambda x: (str(x).lower() == 'true'), default=False)\n",
    "parser.add_argument('--n_interp',           type=int, default=5, \n",
    "                    help='Number of interpolated points inserted between flow points to better approximate MFG costs')\n",
    "## robotics\n",
    "parser.add_argument('--robot_init_pos',     type=str, default='default', choices=['default', \n",
    "                                            'under_table', 'under_table_2', 'under_table_3', 'under_table_4',\n",
    "                                            'under_table_hard']) \n",
    "parser.add_argument('--robot_term_pos',     type=str, default='cup', choices=['cup']) \n",
    "parser.add_argument('--robot_var',          type=float, default=1e-5)\n",
    "parser.add_argument('--robot_obs_val',      type=float, default=1e2)\n",
    "parser.add_argument('--robot_1_obs',        type=str, default='thick_sigmoid_B=256')\n",
    "parser.add_argument('--robot_1_obs_l',      type=int, default=3)\n",
    "parser.add_argument('--robot_1_obs_act',    type=str, default='relu', choices=['relu', 'tanh'])\n",
    "parser.add_argument('--robot_1_base_dist',  type=str, default='default', choices=['default', 'two_init'])\n",
    "parser.add_argument('--obs_robot_sig',      type=lambda x: (str(x).lower() == 'true'), default=True) \n",
    "\n",
    "# misc.\n",
    "parser.add_argument('--plotting_subset',    type=int, default=10000)\n",
    "parser.add_argument('--load_best_val',      type=lambda x: (str(x).lower() == 'true'), default=False)\n",
    "parser.add_argument('--compute_lip_bound',  type=lambda x: (str(x).lower() == 'true'), default=False)\n",
    "parser.add_argument('--save_train_traj',    type=lambda x: (str(x).lower() == 'true'), default=False)\n",
    "parser.add_argument('--syn_noise',          type=float, default=0.1)\n",
    "parser.add_argument('--marker_size',        type=float, default=5)\n",
    "parser.add_argument('--color',              type=str, default='order', choices=[\n",
    "                                            'order', 'radius'])\n",
    "parser.add_argument('--tabular_subset',     type=lambda x: (str(x).lower() == 'true'), default=False)\n",
    "parser.add_argument('--tensor_type',        type=str, default='float', choices=['float', 'double']) \n",
    "\n",
    "args = parser.parse_args()\n",
    "args = sanitize_args(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================== #\n",
    "#                                       Meta                                          #\n",
    "# =================================================================================== #\n",
    "\n",
    "os.environ['DATAROOT'] = 'experiments/dataset/data/'\n",
    "os.environ['SLURM_JOB_ID'] = '1'\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "device = torch.device('cuda')\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 检查CUDA是否可用，然后选择设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Current device:\", device)\n",
    "\n",
    "import torch\n",
    "\n",
    "# 假设 'cuda' 可用\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    generator = torch.Generator(device=device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    generator = torch.Generator(device=device)\n",
    "\n",
    "# 使用这个生成器与 torch.randperm\n",
    "rand_indices = torch.randperm(1, generator=generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================================== #\n",
    "#                                       Dataset                                       #\n",
    "# =================================================================================== #\n",
    "target_dist = None\n",
    "space = None\n",
    "num_mixtures = 8\n",
    "weight = D.Categorical(torch.ones(num_mixtures,).to(device))\n",
    "X_train, _, _, train_loader, val_loader, test_loader, target_dist = make_gaussian_mixture_data(args.mixture_base, args.gaussian_multi_dim, args.num_train_data, \\\n",
    "    args.num_val_data, args.num_test_data, args.train_batch_size, args.val_batch_size, args.test_batch_size, weight=weight)\n",
    "train_generator = data_.batch_generator(train_loader)\n",
    "train_loader\n",
    "test_batch      = next(iter(train_loader)).to(device)\n",
    "features        = args.gaussian_multi_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CompositeTransform(\n",
      "  (_transforms): ModuleList(\n",
      "    (0): LULinear()\n",
      "    (1-2): 2 x PiecewiseRationalQuadraticCouplingTransform(\n",
      "      (transform_net): ResidualNet(\n",
      "        (initial_layer): Linear(in_features=1, out_features=256, bias=True)\n",
      "        (blocks): ModuleList(\n",
      "          (0-1): 2 x ResidualBlock(\n",
      "            (linear_layers): ModuleList(\n",
      "              (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
      "            )\n",
      "            (dropout): Dropout(p=0.25, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (final_layer): Linear(in_features=256, out_features=23, bias=True)\n",
      "      )\n",
      "      (unconditional_transform): PiecewiseRationalQuadraticCDF()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "There are 3774638 trainable parameters in this model.\n"
     ]
    }
   ],
   "source": [
    "# =================================================================================== #\n",
    "#                                       Model                                         #\n",
    "# =================================================================================== #\n",
    "\n",
    "\n",
    "# methods for chaining together the flow transformations\n",
    "def create_linear_transform():\n",
    "    return transforms.LULinear(features, identity_init=True)\n",
    "\n",
    "\n",
    "def create_base_transform(i):\n",
    "    act = F.relu\n",
    "\n",
    "\n",
    "    return transforms.PiecewiseRationalQuadraticCouplingTransform(\n",
    "        mask=utils.create_alternating_binary_mask(features, even=(i % 2 == 0)),\n",
    "        transform_net_create_fn=lambda in_features, out_features: nn_.ResidualNet(\n",
    "            in_features=in_features,\n",
    "            out_features=out_features,\n",
    "            hidden_features=args.hidden_features,\n",
    "            context_features=None,\n",
    "            num_blocks=args.num_transform_blocks,\n",
    "            activation=act,\n",
    "            dropout_probability=args.dropout_probability,\n",
    "            use_batch_norm=args.use_batch_norm\n",
    "        ),\n",
    "        num_bins=args.num_bins,\n",
    "        tails='linear',\n",
    "        tail_bound=args.tail_bound,\n",
    "        apply_unconditional_transform=args.apply_unconditional_transform\n",
    "    )\n",
    "\n",
    "def create_transform():\n",
    "    flows = [\n",
    "        transforms.CompositeTransform([\n",
    "            create_linear_transform(),\n",
    "            create_base_transform(i),\n",
    "            create_base_transform(i+1)\n",
    "        ]) for i in range(0, 2*args.num_flow_steps, 2)\n",
    "    ]\n",
    "\n",
    "    # flows = [\n",
    "    #     transforms.CompositeTransform([\n",
    "    #         create_linear_transform(),\n",
    "    #         create_base_transform(1)\n",
    "    #     ])\n",
    "    # ]\n",
    "\n",
    "    print((flows[0]))\n",
    "    K = args.num_flow_steps\n",
    "    transform = transforms.CompositeTransform(flows)\n",
    "\n",
    "    return transform, K\n",
    "\n",
    "# base dist\n",
    "\n",
    "cov          = 0.0625 * torch.eye(features).to(device)\n",
    "mean         = torch.zeros(features).to(device)\n",
    "distribution = distributions.MultivarNormal((features,), mean=mean, cov=cov)\n",
    "\n",
    "# create flows\n",
    "transform, K = create_transform()\n",
    "flow = flows.Flow(transform, distribution).to(device)\n",
    "n_params = utils.get_num_parameters(flow)\n",
    "print('There are {} trainable parameters in this model.'.format(n_params))\n",
    "\n",
    "# create optimizer\n",
    "optimizer = optim.Adam(flow.parameters(), lr=args.learning_rate, weight_decay=args.lbd_reg)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, verbose=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Flow(\n",
       "  (_transform): CompositeTransform(\n",
       "    (_transforms): ModuleList(\n",
       "      (0-6): 7 x CompositeTransform(\n",
       "        (_transforms): ModuleList(\n",
       "          (0): LULinear()\n",
       "          (1-2): 2 x PiecewiseRationalQuadraticCouplingTransform(\n",
       "            (transform_net): ResidualNet(\n",
       "              (initial_layer): Linear(in_features=1, out_features=256, bias=True)\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x ResidualBlock(\n",
       "                  (linear_layers): ModuleList(\n",
       "                    (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "                  )\n",
       "                  (dropout): Dropout(p=0.25, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (final_layer): Linear(in_features=256, out_features=23, bias=True)\n",
       "            )\n",
       "            (unconditional_transform): PiecewiseRationalQuadraticCDF()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (_distribution): MultivarNormal()\n",
       ")"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(K)\n",
    "flow\n",
    "# \"\"\"\n",
    "# Flow 类是一个包含变换和基础分布的模型。\n",
    "# 它通常用于定义一个可逆的变换链（CompositeTransform），\n",
    "# 以及一个简单的概率分布（_distribution），在这个例子中是多变量正态分布（MultivarNormal()）\n",
    "# \"\"\"\n",
    "\n",
    "# \"\"\"\n",
    "# CompositeTransform：这是一个复合变换，它将多个变换组合在一起。\n",
    "# 在此例子中，它包含了一个模块列表（ModuleList），其中每个模块也是一个 CompositeTransform。\n",
    "\n",
    "# ModuleList (0-9)：ModuleList 包含了10个 CompositeTransform 对象，这表明模型中有10个相似的流程步骤或层，每个都执行相似的变换。\n",
    "# \"\"\"\n",
    "\n",
    "# \"\"\" \n",
    "# 每个 CompositeTransform 包括：\n",
    "\n",
    "# LULinear()：这是一个线性变换，通常用于进行可逆的线性操作，如LU分解。\n",
    "\n",
    "# PiecewiseRationalQuadraticCouplingTransform (1-2)：包括两个此类型的变换，这类变换通常用于在保持一部分输入不变的同时，通过一种复杂的函数（在这里是由 ResidualNet 实现）修改输入的其余部分。\n",
    "\n",
    "#     transform_net：一个 ResidualNet 网络，用于根据输入特征生成变换的参数。这个网络包括：\n",
    "#         initial_layer：输入层，一个线性变换，将输入特征从1维扩展到256维。\n",
    "#         blocks：包含多个 ResidualBlock，每个块包括两个线性层和一个Dropout层，用于增加模型的非线性和防止过拟合。\n",
    "#         final_layer：输出层，另一个线性变换，将特征从256维压缩到23维。\n",
    "#     unconditional_transform：一个 PiecewiseRationalQuadraticCDF，用于在变换中实现无条件的、复杂的可逆函数。\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 设置图形\n",
    "def setup_figure(rows, cols, figsize=(18, 6)):\n",
    "    fig, axs = plt.subplots(rows, cols, figsize=figsize)\n",
    "    return fig, axs\n",
    "\n",
    "# 绘制矢量场\n",
    "def plot_quiver(ax, data, last_data, title):\n",
    "    U = data[:, 0] - last_data[:, 0]\n",
    "    V = data[:, 1] - last_data[:, 1]\n",
    "    ax.quiver(last_data[:, 0], last_data[:, 1], U, V, scale=10, scale_units='width', color='b', width=0.005)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "\n",
    "# 绘制散点图\n",
    "def plot_scatter(ax, data, title):\n",
    "    ax.plot(data[:, 0], data[:, 1], '.')\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "\n",
    "# 绘制密度图\n",
    "def plot_density(ax, data, title):\n",
    "    sns.kdeplot(x=data[:, 0], y=data[:, 1], fill=True, thresh=0, levels=100, cmap=\"viridis\", ax=ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlim(-1.5, 1.5)\n",
    "    ax.set_ylim(-1.5, 1.5)\n",
    "\n",
    "# 主绘图函数\n",
    "def main_plot(part, z_0):\n",
    "    fig1, ax1 = setup_figure(2, 5)\n",
    "    fig2, ax2 = setup_figure(2, 5)\n",
    "    fig3, ax3 = setup_figure(2, 5)\n",
    "\n",
    "    last_x = z_0.data.cpu().numpy()\n",
    "    for i in range(8):\n",
    "        x_plot = part[:, i, :].data.cpu().numpy()\n",
    "        plot_quiver(ax1.flatten()[i], x_plot, last_x, f'Time step {i}')\n",
    "        plot_scatter(ax2.flatten()[i], x_plot, f'Time step {i}')\n",
    "        plot_density(ax3.flatten()[i], x_plot, f'Time step {i}')\n",
    "        last_x = x_plot\n",
    "\n",
    "    for fig in [fig1, fig2, fig3]:\n",
    "        fig.tight_layout()\n",
    "        fig.show()\n",
    "\n",
    "\n",
    "# z_K, ld_gen, OT_cost_gen, hist_gen, hist_ld_gen, z_0 = flow.sample(args.train_batch_size)\n",
    "# hist = hist_gen\n",
    "# part = torch.cat(hist).reshape(len(hist), hist[0].shape[0], hist[0].shape[-1]).permute(1,0,2)\n",
    "# K = part.shape[1]\n",
    "# I = torch.arange(0, K, step=4)\n",
    "# part = part[:,I,:]\n",
    "\n",
    "# main_plot(part, z_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/10 [00:00<?, ?iter/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/10 [00:00<?, ?iter/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  4,  8, 12, 16, 20, 24, 28])\n",
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Energy() missing 1 required positional argument: 'logp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 85\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# grab data\u001b[39;00m\n\u001b[1;32m     84\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(train_generator)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 85\u001b[0m loss_dict \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# 保存loss值\u001b[39;00m\n\u001b[1;32m     87\u001b[0m losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(loss_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[99], line 51\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(flow, args, data, mode)\u001b[0m\n\u001b[1;32m     49\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(K):\n\u001b[0;32m---> 51\u001b[0m     Energy_seq[i] \u001b[38;5;241m=\u001b[39m \u001b[43mEnergy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpart\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m     arclength2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(torch\u001b[38;5;241m.\u001b[39mnorm(part[:,i,:] \u001b[38;5;241m-\u001b[39m part[:,i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,:], dim\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     53\u001b[0m     arclength_seq[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39marclength2\n",
      "\u001b[0;31mTypeError\u001b[0m: Energy() missing 1 required positional argument: 'logp'"
     ]
    }
   ],
   "source": [
    "# =================================================================================== #\n",
    "#                                      Training                                       #\n",
    "# =================================================================================== #\n",
    "beta = 0.5\n",
    "def Energy(x, logp):\n",
    "    energy = 0\n",
    "\n",
    "    potential = (x[:,0] - 1)**2 + (x[:,1] - 1)**2\n",
    "    energy = torch.mean(potential) + 1/beta * torch.mean(logp)\n",
    "    return energy\n",
    "\n",
    "def GradF2(x, logp):\n",
    "    x = torch.squeeze(x)\n",
    "    N = x.shape[0]\n",
    "    energy = Energy(x, logp) * N\n",
    "    grad = torch.autograd.grad(outputs=energy, inputs=x, create_graph=True, grad_outputs=torch.ones_like(energy))[0]\n",
    "    gradF2 = (torch.sum(grad ** 2) * 1/N)\n",
    "    return gradF2\n",
    "\n",
    "def compute_loss(flow,  args, data=None, mode='train'):\n",
    "\n",
    "\n",
    "\n",
    "    z_K, ld_gen, OT_cost_gen, hist_gen, hist_ld_gen, z_0 = flow.sample(args.train_batch_size)\n",
    "    log_density, logp0, _, hist_norm, hist_ld_norm, OT_cost_norm, _ = flow.log_prob(z_K)\n",
    "    G_cost =  Energy(z_K, log_density) -torch.mean(log_density)\n",
    "    \n",
    "    \n",
    "\n",
    "    # OT regularization (L)\n",
    "    L_cost = torch.Tensor([0]).to(device)\n",
    "    \n",
    "    # Running cost\n",
    "    hist   = hist_norm\n",
    "    part = torch.cat(hist).reshape(len(hist), hist[0].shape[0], hist[0].shape[-1]).permute(1,0,2) # B x K x d\n",
    "    K    = part.shape[1]\n",
    "    I = torch.arange(0, K, step=4)\n",
    "    print(I)\n",
    "    part = part[:,I,:]\n",
    "    K = part.shape[1]-1\n",
    "    print(K)\n",
    "    I = torch.arange(K)\n",
    "    \n",
    "    Energy_seq = torch.zeros(K+1)\n",
    "    Energy_seq[0]=Energy(part[:,0,:], log_density)   \n",
    "    arclength_seq = torch.zeros(K)\n",
    "    action_seq = torch.zeros(K)\n",
    "    cosvalue = torch.zeros(K)\n",
    "    running_loss = 0\n",
    "    for i in range(K):\n",
    "        Energy_seq[i] = Energy(part[:,i,:])\n",
    "        arclength2 = torch.mean(torch.norm(part[:,i,:] - part[:,i-1,:], dim=[1])**2)\n",
    "        arclength_seq[i-1] =arclength2\n",
    "        gradF2 = GradF2(part[:,i,:])\n",
    "        running_loss += torch.sqrt(gradF2 * arclength2)\n",
    "        action_seq[i-1] = torch.sqrt(gradF2 * arclength2)\n",
    "        cosvalue[i-1] = torch.sqrt((Energy_seq[i]-Energy_seq[i-1])**2 / (gradF2 * arclength2) )\n",
    "    L = torch.mean(K * torch.norm(part[:,I,:] - part[:,I+1,:], dim=[1,2])**2)\n",
    "    differences = Energy_seq[1:] - Energy_seq[:-1]\n",
    "    energydiff_loss = (torch.max(differences)/torch.min(differences)-1)**2\n",
    "    arclendiff_loss = (torch.max(arclength_seq)/torch.min(arclength_seq)-1)**2\n",
    "    arcactdiff_loss = (torch.max(action_seq)/torch.min(action_seq)-1)**2\n",
    "    # interaction (F)\n",
    "    F_P    = torch.Tensor([0]).to(device)\n",
    "    F_E    = torch.Tensor([0]).to(device)\n",
    "    F_cost = torch.Tensor([0]).to(device)\n",
    "    \n",
    "    # Overall loss\n",
    "    loss = G_cost + args.lbd_OT * (L)\n",
    "\n",
    "    # return loss, G_cost, L_cost, F_cost\n",
    "    return {'loss': loss, 'Terminal': G_cost, 'Running_OT': L, 'Running_action':running_loss, 'arc': (arcactdiff_loss), 'F': F_cost, 'Energy_seq': Energy_seq, 'arclength_seq': arclength_seq, 'action_seq': action_seq, 'cosvalue': cosvalue}\n",
    "\n",
    "# main loop\n",
    "# 初始化存储损失值的字典\n",
    "losses = {'loss': [], 'Terminal': [], 'Running_OT': [], 'Running_action': [], 'arc': []}\n",
    "\n",
    "args.num_training_steps = 10\n",
    "for step in tqdm(range(args.num_training_steps), desc='Training Progress', unit='iter'):\n",
    "    flow.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # grab data\n",
    "    batch = next(train_generator).to(device)\n",
    "    loss_dict = compute_loss(flow, args, mode='train')\n",
    "    # 保存loss值\n",
    "    losses['loss'].append(loss_dict['loss'].item())\n",
    "    losses['Terminal'].append(loss_dict['Terminal'].item())\n",
    "    losses['Running_OT'].append(loss_dict['Running_OT'].item())\n",
    "    losses['Running_action'].append(loss_dict['Running_action'].item())\n",
    "    losses['arc'].append(loss_dict['arc'].item())\n",
    "\n",
    "\n",
    "    loss      = loss_dict['loss']\n",
    "\n",
    "    loss.backward()\n",
    "    clip_grad_norm_(flow.parameters(), args.grad_norm_clip_value)\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# 画图部分\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "loss_types = ['loss', 'Terminal', 'Running_OT', 'Running_action','arc']\n",
    "titles = ['loss', 'Terminal', 'Running_OT', 'Running_action','arc']\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.plot(losses[loss_types[i]], label=loss_types[i])\n",
    "    ax.set_title(titles[i])\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "\n",
    "    # 标注最后一点的值\n",
    "    final_value = losses[loss_types[i]][-1]\n",
    "    ax.annotate(f'{final_value:.2f}', # 格式化为两位小数\n",
    "                xy=(len(losses[loss_types[i]]) - 1, final_value),\n",
    "                xytext=(8, 0), \n",
    "                textcoords='offset points',\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('running_loss',losses['Running_action'][-1])\n",
    "\n",
    "loss_dict = compute_loss(flow, points, args, mode='train')\n",
    "loss_types = ['Energy_seq', 'arclength_seq', 'action_seq', 'cosvalue']\n",
    "titles = ['Energy Sequence', 'Arclength Sequence', 'Action Sequence', 'cosvalue']\n",
    "\n",
    "# 创建4个子图\n",
    "fig, axs = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "# 遍历每个子图并绘制相应的loss数据\n",
    "for i, ax in enumerate(axs):\n",
    "    ax.plot(loss_dict[loss_types[i]].data.cpu().numpy(), marker='o')\n",
    "    ax.set_title(titles[i])\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print('energy difference', loss_dict['Energy_seq'].max()-loss_dict['Energy_seq'].min())\n",
    "\n",
    "z_K, ld_gen, OT_cost_gen, hist_gen, hist_ld_gen, z_0 = flow.sample(args.train_batch_size)\n",
    "hist = hist_gen\n",
    "part = torch.cat(hist).reshape(len(hist), hist[0].shape[0], hist[0].shape[-1]).permute(1,0,2)\n",
    "K = part.shape[1]\n",
    "I = torch.arange(0, K, step=4)\n",
    "part = part[:,I,:]\n",
    "\n",
    "main_plot(part, z_0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
